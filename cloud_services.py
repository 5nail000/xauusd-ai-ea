"""
–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –æ–±–ª–∞—á–Ω—ã–º–∏ —Å–µ—Ä–≤–∏—Å–∞–º–∏ (Hugging Face):
- –†–∞–±–æ—Ç–∞ —Å —Ç–∏–∫–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
- –ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π (--analyze-features)
"""
import os
import tarfile
import argparse
from pathlib import Path
from datetime import datetime
import subprocess
import sys
from typing import Optional, List, Dict, Any
import shutil
import time
import json

try:
    from huggingface_hub import HfApi, upload_folder, snapshot_download
    HF_AVAILABLE = True
except ImportError:
    HF_AVAILABLE = False
    print("‚ö†Ô∏è  huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")

try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False
    print("‚ö†Ô∏è  requests –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install requests")

def get_directory_size(path: Path) -> int:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –≤ –±–∞–π—Ç–∞—Ö"""
    total = 0
    for dirpath, dirnames, filenames in os.walk(path):
        for filename in filenames:
            filepath = os.path.join(dirpath, filename)
            if os.path.exists(filepath):
                total += os.path.getsize(filepath)
    return total


def format_size(size_bytes: int) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–∞–∑–º–µ—Ä –≤ —á–∏—Ç–∞–µ–º—ã–π –≤–∏–¥"""
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if size_bytes < 1024.0:
            return f"{size_bytes:.2f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.2f} PB"


class HuggingFaceUploader:
    """–ö–ª–∞—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ Hugging Face Hub"""
    
    def __init__(self, repo_id: str, token: Optional[str] = None):
        """
        Args:
            repo_id: ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –Ω–∞ Hugging Face (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'username/dataset-name')
            token: Hugging Face —Ç–æ–∫–µ–Ω (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è HF_TOKEN)
        """
        if not HF_AVAILABLE:
            raise ImportError("huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
        
        self.repo_id = repo_id
        self.api = HfApi(token=token)
        self.token = token or os.getenv('HF_TOKEN')
        
        if not self.token:
            print("‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: HF_TOKEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ú–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è.")
    
    def upload_hf_ticks(self, ticks_dir: str = 'workspace/raw_data/ticks', 
                     commit_message: str = "Upload tick data") -> bool:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–∏–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ Hugging Face
        
        Args:
            ticks_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ç–∏–∫–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            commit_message: –°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞
        """
        print("=" * 60)
        print("–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–∏–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ Hugging Face")
        print("=" * 60)
        
        ticks_path = Path(ticks_dir)
        if not ticks_path.exists():
            print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {ticks_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
            return False
        
        size = get_directory_size(ticks_path)
        print(f"üìä –†–∞–∑–º–µ—Ä —Ç–∏–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {format_size(size)}")
        print(f"üìÅ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {self.repo_id}")
        print(f"üìÇ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {ticks_dir}")
        
        try:
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            temp_dir = Path('temp_hf_upload')
            temp_ticks_dir = temp_dir / 'ticks'
            temp_ticks_dir.mkdir(parents=True, exist_ok=True)
            
            # –ö–æ–ø–∏—Ä—É–µ–º —Ç–∏–∫–∏ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            print(f"\n–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
            shutil.copytree(ticks_path, temp_ticks_dir, dirs_exist_ok=True)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞ Hugging Face
            print(f"\n–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ Hugging Face...")
            upload_folder(
                folder_path=str(temp_dir),
                repo_id=self.repo_id,
                repo_type="dataset",
                token=self.token,
                commit_message=commit_message
            )
            
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            shutil.rmtree(temp_dir, ignore_errors=True)
            
            print(f"\n‚úì –¢–∏–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –Ω–∞ Hugging Face!")
            print(f"  –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: https://huggingface.co/datasets/{self.repo_id}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
            shutil.rmtree(temp_dir, ignore_errors=True)
            return False
    
    def upload_hf_feature_analysis(self, 
                               analysis_dir: str = 'workspace/analysis-of-features',
                               commit_message: str = "Upload feature analysis results") -> bool:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π (--analyze-features) –Ω–∞ Hugging Face
        
        Args:
            analysis_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: workspace/analysis-of-features)
            commit_message: –°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞
        """
        print("=" * 60)
        print("–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π –Ω–∞ Hugging Face")
        print("=" * 60)
        
        analysis_path = Path(analysis_dir)
        if not analysis_path.exists():
            print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {analysis_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
            print("   –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python full_pipeline.py --analyze-features")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        expected_files = [
            'feature_statistics.csv',
            'feature_importance.csv',
            'outliers_analysis.csv',
            'feature_by_class_statistics.csv',
            'feature_analysis_report.html'
        ]
        
        found_files = []
        for file_name in expected_files:
            file_path = analysis_path / file_name
            if file_path.exists():
                found_files.append(file_name)
                size = file_path.stat().st_size
                print(f"‚úì –ù–∞–π–¥–µ–Ω: {file_name} ({format_size(size)})")
            else:
                print(f"‚ö† –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç: {file_name}")
        
        if not found_files:
            print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞!")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ plots (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        plots_dir = analysis_path / 'plots'
        has_plots = plots_dir.exists() and plots_dir.is_dir()
        if has_plots:
            plots_size = get_directory_size(plots_dir)
            print(f"‚úì –ù–∞–π–¥–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è plots ({format_size(plots_size)})")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ excluded_features.txt –≤ workspace
        excluded_features_file = Path('workspace/excluded_features.txt')
        has_excluded = excluded_features_file.exists()
        if has_excluded:
            size = excluded_features_file.stat().st_size
            print(f"‚úì –ù–∞–π–¥–µ–Ω: excluded_features.txt ({format_size(size)})")
        
        size = get_directory_size(analysis_path)
        print(f"\nüìä –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {format_size(size)}")
        print(f"üìÅ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {self.repo_id}")
        print(f"üìÇ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {analysis_dir}")
        
        try:
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            temp_dir = Path('temp_hf_upload')
            temp_analysis_dir = temp_dir / 'analysis-of-features'
            temp_analysis_dir.mkdir(parents=True, exist_ok=True)
            
            # –ö–æ–ø–∏—Ä—É–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏–∑ analysis_dir
            print(f"\n–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
            for item in analysis_path.iterdir():
                if item.is_file():
                    shutil.copy2(item, temp_analysis_dir / item.name)
                    print(f"  –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω —Ñ–∞–π–ª: {item.name}")
                elif item.is_dir():
                    shutil.copytree(item, temp_analysis_dir / item.name, dirs_exist_ok=True)
                    print(f"  –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {item.name}")
            
            # –ö–æ–ø–∏—Ä—É–µ–º excluded_features.txt –∏–∑ workspace, –µ—Å–ª–∏ –æ–Ω —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            if has_excluded:
                temp_workspace_dir = temp_dir / 'workspace'
                temp_workspace_dir.mkdir(parents=True, exist_ok=True)
                shutil.copy2(excluded_features_file, temp_workspace_dir / 'excluded_features.txt')
                print(f"  –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω —Ñ–∞–π–ª: workspace/excluded_features.txt")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞ Hugging Face
            print(f"\n–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ Hugging Face...")
            upload_folder(
                folder_path=str(temp_dir),
                repo_id=self.repo_id,
                repo_type="dataset",
                token=self.token,
                commit_message=commit_message
            )
            
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            shutil.rmtree(temp_dir, ignore_errors=True)
            
            print(f"\n‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –Ω–∞ Hugging Face!")
            print(f"  –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: https://huggingface.co/datasets/{self.repo_id}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
            import traceback
            traceback.print_exc()
            shutil.rmtree(temp_dir, ignore_errors=True)
            return False
    
    def upload_hf_training_data(self,
                             include_scalers: bool = True,
                             include_cache: bool = False,
                             commit_message: str = "Upload training data") -> bool:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ Hugging Face (–±–µ–∑ —Ç–∏–∫–æ–≤)
        
        Args:
            include_scalers: –í–∫–ª—é—á–∞—Ç—å –ª–∏ scalers
            include_cache: –í–∫–ª—é—á–∞—Ç—å –ª–∏ –∫—ç—à–∏
            commit_message: –°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞
        """
        print("=" * 60)
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ Hugging Face")
        print("=" * 60)
        
        paths_to_include = []
        
        # CSV —Ñ–∞–π–ª—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)
        csv_files = [
            'workspace/prepared/features/gold_train.csv',
            'workspace/prepared/features/gold_val.csv',
            'workspace/prepared/features/gold_test.csv'
        ]
        
        for csv_file in csv_files:
            csv_path = Path(csv_file)
            if csv_path.exists():
                paths_to_include.append(csv_file)
                size = csv_path.stat().st_size
                print(f"‚úì –í–∫–ª—é—á–µ–Ω: {csv_file} ({format_size(size)})")
            else:
                print(f"‚ö† –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: {csv_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # Scalers (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if include_scalers:
            scalers_dir = Path('workspace/prepared/scalers')
            if scalers_dir.exists():
                size = get_directory_size(scalers_dir)
                paths_to_include.append(str(scalers_dir))
                print(f"‚úì –í–∫–ª—é—á–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {scalers_dir} ({format_size(size)})")
        
        # –ö—ç—à–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if include_cache:
            cache_dir = Path('workspace/raw_data/cache')
            if cache_dir.exists():
                size = get_directory_size(cache_dir)
                paths_to_include.append(str(cache_dir))
                print(f"‚úì –í–∫–ª—é—á–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {cache_dir} ({format_size(size)})")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ excluded_features.txt –≤ workspace
        excluded_features_file = Path('workspace/excluded_features.txt')
        has_excluded = excluded_features_file.exists()
        if has_excluded:
            size = excluded_features_file.stat().st_size
            print(f"‚úì –ù–∞–π–¥–µ–Ω: excluded_features.txt ({format_size(size)})")
        
        if not paths_to_include:
            print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏!")
            return False
        
        print(f"\nüìÅ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {self.repo_id}")
        
        try:
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            temp_dir = Path('temp_hf_upload')
            temp_dir.mkdir(parents=True, exist_ok=True)
            
            # –ö–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            print(f"\n–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
            for path_str in paths_to_include:
                path = Path(path_str)
                if path.exists():
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
                    rel_path = path.relative_to(Path('workspace').parent)
                    dest_path = temp_dir / rel_path
                    dest_path.parent.mkdir(parents=True, exist_ok=True)
                    
                    if path.is_file():
                        shutil.copy2(path, dest_path)
                    else:
                        shutil.copytree(path, dest_path, dirs_exist_ok=True)
                    print(f"  –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ: {path_str}")
            
            # –ö–æ–ø–∏—Ä—É–µ–º excluded_features.txt, –µ—Å–ª–∏ –æ–Ω —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            if has_excluded:
                workspace_dest = temp_dir / 'workspace'
                workspace_dest.mkdir(parents=True, exist_ok=True)
                shutil.copy2(excluded_features_file, workspace_dest / 'excluded_features.txt')
                print(f"  –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ: workspace/excluded_features.txt")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞ Hugging Face
            print(f"\n–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ Hugging Face...")
            upload_folder(
                folder_path=str(temp_dir),
                repo_id=self.repo_id,
                repo_type="dataset",
                token=self.token,
                commit_message=commit_message
            )
            
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            shutil.rmtree(temp_dir, ignore_errors=True)
            
            print(f"\n‚úì –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –Ω–∞ Hugging Face!")
            print(f"  –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: https://huggingface.co/datasets/{self.repo_id}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
            import traceback
            traceback.print_exc()
            shutil.rmtree(temp_dir, ignore_errors=True)
            return False
    
    # –ê–ª–∏–∞—Å—ã –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    upload_ticks = upload_hf_ticks
    upload_feature_analysis = upload_hf_feature_analysis
    upload_training_data = upload_hf_training_data


class HuggingFaceDeleter:
    """–ö–ª–∞—Å—Å –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ Hugging Face Hub"""
    
    def __init__(self, repo_id: str, token: Optional[str] = None):
        """
        Args:
            repo_id: ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –Ω–∞ Hugging Face (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'username/dataset-name')
            token: Hugging Face —Ç–æ–∫–µ–Ω (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è HF_TOKEN)
        """
        if not HF_AVAILABLE:
            raise ImportError("huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
        
        self.repo_id = repo_id
        self.api = HfApi(token=token)
        self.token = token or os.getenv('HF_TOKEN')
        
        if not self.token:
            print("‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: HF_TOKEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ú–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è.")
    
    def _list_repo_files(self) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏"""
        try:
            files = self.api.list_repo_files(
                repo_id=self.repo_id,
                repo_type="dataset",
                token=self.token
            )
            return files
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤: {e}")
            return []
    
    def delete_hf_ticks(self, commit_message: str = "Delete tick data") -> bool:
        """
        –£–¥–∞–ª—è–µ—Ç —Ç–∏–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
        
        Args:
            commit_message: –°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞
        """
        print("=" * 60)
        print("–£–¥–∞–ª–µ–Ω–∏–µ —Ç–∏–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ Hugging Face")
        print("=" * 60)
        print(f"üìÅ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {self.repo_id}")
        
        try:
            files = self._list_repo_files()
            tick_files = [f for f in files if f.startswith('ticks/')]
            
            if not tick_files:
                print("‚úì –¢–∏–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏")
                return True
            
            print(f"\n–ù–∞–π–¥–µ–Ω–æ {len(tick_files)} —Ñ–∞–π–ª–æ–≤/–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è:")
            for file in tick_files[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
                print(f"  - {file}")
            if len(tick_files) > 10:
                print(f"  ... –∏ –µ—â–µ {len(tick_files) - 10} —Ñ–∞–π–ª–æ–≤")
            
            # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª—ã
            print(f"\n–£–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤...")
            for file in tick_files:
                try:
                    self.api.delete_file(
                        path_in_repo=file,
                        repo_id=self.repo_id,
                        repo_type="dataset",
                        token=self.token,
                        commit_message=commit_message if file == tick_files[0] else None
                    )
                except Exception as e:
                    print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ {file}: {e}")
            
            print(f"\n‚úì –¢–∏–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω—ã –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è!")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def delete_hf_training_data(self, 
                               include_scalers: bool = True,
                               include_cache: bool = True,
                               commit_message: str = "Delete training data") -> bool:
        """
        –£–¥–∞–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
        
        Args:
            include_scalers: –£–¥–∞–ª—è—Ç—å –ª–∏ scalers
            include_cache: –£–¥–∞–ª—è—Ç—å –ª–∏ –∫—ç—à–∏
            commit_message: –°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞
        """
        print("=" * 60)
        print("–£–¥–∞–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏–∑ Hugging Face")
        print("=" * 60)
        print(f"üìÅ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {self.repo_id}")
        
        try:
            files = self._list_repo_files()
            files_to_delete = []
            
            # CSV —Ñ–∞–π–ª—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            training_patterns = [
                'workspace/prepared/features/gold_train.csv',
                'workspace/prepared/features/gold_val.csv',
                'workspace/prepared/features/gold_test.csv'
            ]
            
            # Scalers
            if include_scalers:
                scaler_files = [f for f in files if f.startswith('workspace/prepared/scalers/')]
                files_to_delete.extend(scaler_files)
            
            # –ö—ç—à–∏
            if include_cache:
                cache_files = [f for f in files if f.startswith('workspace/raw_data/cache/')]
                files_to_delete.extend(cache_files)
            
            # CSV —Ñ–∞–π–ª—ã
            for pattern in training_patterns:
                if pattern in files:
                    files_to_delete.append(pattern)
            
            # excluded_features.txt
            excluded_file = 'workspace/excluded_features.txt'
            if excluded_file in files:
                files_to_delete.append(excluded_file)
            
            if not files_to_delete:
                print("‚úì –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏")
                return True
            
            print(f"\n–ù–∞–π–¥–µ–Ω–æ {len(files_to_delete)} —Ñ–∞–π–ª–æ–≤/–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è:")
            for file in files_to_delete[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
                print(f"  - {file}")
            if len(files_to_delete) > 10:
                print(f"  ... –∏ –µ—â–µ {len(files_to_delete) - 10} —Ñ–∞–π–ª–æ–≤")
            
            # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª—ã
            print(f"\n–£–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤...")
            for i, file in enumerate(files_to_delete):
                try:
                    self.api.delete_file(
                        path_in_repo=file,
                        repo_id=self.repo_id,
                        repo_type="dataset",
                        token=self.token,
                        commit_message=commit_message if i == 0 else None
                    )
                except Exception as e:
                    print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ {file}: {e}")
            
            print(f"\n‚úì –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω—ã –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è!")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def delete_hf_feature_analysis(self, commit_message: str = "Delete feature analysis results") -> bool:
        """
        –£–¥–∞–ª—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
        
        Args:
            commit_message: –°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞
        """
        print("=" * 60)
        print("–£–¥–∞–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π –∏–∑ Hugging Face")
        print("=" * 60)
        print(f"üìÅ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {self.repo_id}")
        
        try:
            files = self._list_repo_files()
            analysis_files = [f for f in files if f.startswith('analysis-of-features/')]
            
            if not analysis_files:
                print("‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏")
                return True
            
            print(f"\n–ù–∞–π–¥–µ–Ω–æ {len(analysis_files)} —Ñ–∞–π–ª–æ–≤/–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è:")
            for file in analysis_files[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
                print(f"  - {file}")
            if len(analysis_files) > 10:
                print(f"  ... –∏ –µ—â–µ {len(analysis_files) - 10} —Ñ–∞–π–ª–æ–≤")
            
            # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª—ã
            print(f"\n–£–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤...")
            for i, file in enumerate(analysis_files):
                try:
                    self.api.delete_file(
                        path_in_repo=file,
                        repo_id=self.repo_id,
                        repo_type="dataset",
                        token=self.token,
                        commit_message=commit_message if i == 0 else None
                    )
                except Exception as e:
                    print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ {file}: {e}")
            
            print(f"\n‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω—ã –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è!")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def delete_all_data(self, commit_message: str = "Delete all dataset data") -> bool:
        """
        –£–¥–∞–ª—è–µ—Ç –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–æ—á–∏—â–∞–µ—Ç —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –¥–ª—è –Ω–æ–≤—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫)
        
        Args:
            commit_message: –°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞
        """
        print("=" * 60)
        print("–£–¥–∞–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ Hugging Face")
        print("=" * 60)
        print(f"üìÅ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {self.repo_id}")
        print("‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –≠—Ç–æ —É–¥–∞–ª–∏—Ç –í–°–ï –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è!")
        
        try:
            files = self._list_repo_files()
            
            if not files:
                print("‚úì –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π —É–∂–µ –ø—É—Å—Ç")
                return True
            
            print(f"\n–ù–∞–π–¥–µ–Ω–æ {len(files)} —Ñ–∞–π–ª–æ–≤/–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è:")
            for file in files[:20]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 20
                print(f"  - {file}")
            if len(files) > 20:
                print(f"  ... –∏ –µ—â–µ {len(files) - 20} —Ñ–∞–π–ª–æ–≤")
            
            # –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
            response = input("\n–í—ã —É–≤–µ—Ä–µ–Ω—ã, —á—Ç–æ —Ö–æ—Ç–∏—Ç–µ —É–¥–∞–ª–∏—Ç—å –í–°–ï –¥–∞–Ω–Ω—ã–µ? (yes/no): ").strip().lower()
            if response != 'yes':
                print("‚ùå –£–¥–∞–ª–µ–Ω–∏–µ –æ—Ç–º–µ–Ω–µ–Ω–æ")
                return False
            
            # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª—ã
            print(f"\n–£–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤...")
            for i, file in enumerate(files):
                try:
                    self.api.delete_file(
                        path_in_repo=file,
                        repo_id=self.repo_id,
                        repo_type="dataset",
                        token=self.token,
                        commit_message=commit_message if i == 0 else None
                    )
                    if (i + 1) % 10 == 0:
                        print(f"  –£–¥–∞–ª–µ–Ω–æ {i + 1}/{len(files)} —Ñ–∞–π–ª–æ–≤...")
                except Exception as e:
                    print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ {file}: {e}")
            
            print(f"\n‚úì –í—Å–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω—ã –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è!")
            print(f"  –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –≥–æ—Ç–æ–≤ –¥–ª—è –Ω–æ–≤—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏: {e}")
            import traceback
            traceback.print_exc()
            return False


class HuggingFaceDownloader:
    """–ö–ª–∞—Å—Å –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —Å Hugging Face Hub"""
    
    def __init__(self, repo_id: str, token: Optional[str] = None):
        """
        Args:
            repo_id: ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –Ω–∞ Hugging Face (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'username/dataset-name')
            token: Hugging Face —Ç–æ–∫–µ–Ω (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è HF_TOKEN)
        """
        if not HF_AVAILABLE:
            raise ImportError("huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
        
        self.repo_id = repo_id
        self.api = HfApi(token=token)
        self.token = token or os.getenv('HF_TOKEN')
    
    def download_hf_ticks(self, local_dir: str = 'workspace/raw_data/ticks') -> bool:
        """
        –°–∫–∞—á–∏–≤–∞–µ—Ç —Ç–∏–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å Hugging Face
        
        Args:
            local_dir: –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        print("=" * 60)
        print("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ç–∏–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å Hugging Face")
        print("=" * 60)
        
        print(f"üìÅ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {self.repo_id}")
        print(f"üìÇ –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {local_dir}")
        
        try:
            local_path = Path(local_dir)
            local_path.mkdir(parents=True, exist_ok=True)
            
            # –°–∫–∞—á–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–∞–ø–∫—É ticks
            print(f"\n–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (—Ç–æ–ª—å–∫–æ –ø–∞–ø–∫–∞ ticks/)...")
            temp_dir = Path('temp_hf_download')
            temp_dir.mkdir(exist_ok=True)
            
            try:
                downloaded_path = snapshot_download(
                    repo_id=self.repo_id,
                    repo_type="dataset",
                    local_dir=str(temp_dir),
                    token=self.token,
                    allow_patterns=["ticks/**"]  # –°–∫–∞—á–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–∞–ø–∫—É ticks
                )
                
                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                downloaded_path = Path(downloaded_path)
                ticks_source = downloaded_path / 'ticks'
                
                if ticks_source.exists() and ticks_source.is_dir():
                    print(f"  –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏...")
                    for item in ticks_source.iterdir():
                        dest = local_path / item.name
                        if item.is_file():
                            if dest.exists():
                                dest.unlink()
                            shutil.move(str(item), str(dest))
                        else:
                            if dest.exists():
                                shutil.rmtree(dest)
                            shutil.move(str(item), str(dest))
                else:
                    print("‚ö†Ô∏è  –ü–∞–ø–∫–∞ ticks –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏")
                    print(f"   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è. –û–∂–∏–¥–∞–µ—Ç—Å—è: ticks/")
                    return False
            finally:
                # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
                if temp_dir.exists():
                    shutil.rmtree(temp_dir, ignore_errors=True)
            
            print(f"\n‚úì –¢–∏–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω—ã!")
            print(f"  –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {local_dir}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def download_hf_training_data(self, local_dir: str = 'workspace') -> bool:
        """
        –°–∫–∞—á–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å Hugging Face
        
        Args:
            local_dir: –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        print("=" * 60)
        print("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å Hugging Face")
        print("=" * 60)
        
        print(f"üìÅ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {self.repo_id}")
        print(f"üìÇ –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {local_dir}")
        
        try:
            local_path = Path(local_dir)
            local_path.mkdir(parents=True, exist_ok=True)
            
            # –°–∫–∞—á–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (CSV —Ñ–∞–π–ª—ã, scalers, cache, excluded_features.txt)
            print(f"\n–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (—Ç–æ–ª—å–∫–æ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è)...")
            temp_dir = Path('temp_hf_download')
            temp_dir.mkdir(exist_ok=True)
            
            try:
                downloaded_path = snapshot_download(
                    repo_id=self.repo_id,
                    repo_type="dataset",
                    local_dir=str(temp_dir),
                    token=self.token,
                    allow_patterns=[
                        "workspace/prepared/features/*.csv",
                        "workspace/prepared/scalers/**",
                        "workspace/raw_data/cache/**",
                        "workspace/excluded_features.txt"  # –°–∫–∞—á–∏–≤–∞–µ–º excluded_features.txt
                    ]  # –°–∫–∞—á–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                )
                
                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, —Å–æ—Ö—Ä–∞–Ω—è—è —Å—Ç—Ä—É–∫—Ç—É—Ä—É
                downloaded_path = Path(downloaded_path)
                workspace_source = downloaded_path / 'workspace'
                
                if workspace_source.exists():
                    print(f"  –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏...")
                    found_any = False
                    
                    # –ü–µ—Ä–µ–º–µ—â–∞–µ–º prepared/features/*.csv
                    features_source = workspace_source / 'prepared' / 'features'
                    if features_source.exists():
                        features_dest = local_path / 'prepared' / 'features'
                        features_dest.mkdir(parents=True, exist_ok=True)
                        csv_files = list(features_source.glob('*.csv'))
                        if csv_files:
                            found_any = True
                            for csv_file in csv_files:
                                dest_file = features_dest / csv_file.name
                                if dest_file.exists():
                                    dest_file.unlink()
                                shutil.move(str(csv_file), str(dest_file))
                    
                    # –ü–µ—Ä–µ–º–µ—â–∞–µ–º prepared/scalers
                    scalers_source = workspace_source / 'prepared' / 'scalers'
                    if scalers_source.exists():
                        found_any = True
                        scalers_dest = local_path / 'prepared' / 'scalers'
                        if scalers_dest.exists():
                            shutil.rmtree(scalers_dest)
                        scalers_dest.parent.mkdir(parents=True, exist_ok=True)
                        shutil.move(str(scalers_source), str(scalers_dest))
                    
                    # –ü–µ—Ä–µ–º–µ—â–∞–µ–º raw_data/cache
                    cache_source = workspace_source / 'raw_data' / 'cache'
                    if cache_source.exists():
                        found_any = True
                        cache_dest = local_path / 'raw_data' / 'cache'
                        if cache_dest.exists():
                            shutil.rmtree(cache_dest)
                        cache_dest.parent.mkdir(parents=True, exist_ok=True)
                        shutil.move(str(cache_source), str(cache_dest))
                    
                    # –ü–µ—Ä–µ–º–µ—â–∞–µ–º excluded_features.txt, –µ—Å–ª–∏ –æ–Ω –Ω–∞–π–¥–µ–Ω
                    excluded_source = workspace_source / 'excluded_features.txt'
                    if excluded_source.exists():
                        excluded_dest = local_path / 'excluded_features.txt'
                        if excluded_dest.exists():
                            excluded_dest.unlink()
                        excluded_dest.parent.mkdir(parents=True, exist_ok=True)
                        shutil.move(str(excluded_source), str(excluded_dest))
                        print(f"  ‚úì –ü–µ—Ä–µ–º–µ—â–µ–Ω excluded_features.txt –≤ workspace/")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ö–æ—Ç—è –±—ã —á—Ç–æ-—Ç–æ –±—ã–ª–æ —Å–∫–∞—á–∞–Ω–æ
                    if not found_any:
                        print("‚ö†Ô∏è  –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏")
                        print(f"   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è.")
                        print(f"   –û–∂–∏–¥–∞–µ—Ç—Å—è: workspace/prepared/features/*.csv –∏–ª–∏ workspace/prepared/scalers/")
                        return False
                else:
                    print("‚ö†Ô∏è  –°—Ç—Ä—É–∫—Ç—É—Ä–∞ workspace –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏")
                    print(f"   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è. –û–∂–∏–¥–∞–µ—Ç—Å—è: workspace/")
                    return False
            finally:
                # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
                if temp_dir.exists():
                    shutil.rmtree(temp_dir, ignore_errors=True)
            
            print(f"\n‚úì –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω—ã!")
            print(f"  –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {local_dir}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def download_hf_feature_analysis(self, local_dir: str = 'workspace/analysis-of-features') -> bool:
        """
        –°–∫–∞—á–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π —Å Hugging Face
        
        Args:
            local_dir: –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: workspace/analysis-of-features)
        """
        print("=" * 60)
        print("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π —Å Hugging Face")
        print("=" * 60)
        
        print(f"üìÅ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {self.repo_id}")
        print(f"üìÇ –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {local_dir}")
        
        try:
            local_path = Path(local_dir)
            local_path.mkdir(parents=True, exist_ok=True)
            
            # –°–∫–∞—á–∏–≤–∞–µ–º –ø–∞–ø–∫—É analysis-of-features –∏ excluded_features.txt
            print(f"\n–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (–ø–∞–ø–∫–∞ analysis-of-features/ –∏ excluded_features.txt)...")
            temp_dir = Path('temp_hf_download')
            temp_dir.mkdir(exist_ok=True)
            
            try:
                downloaded_path = snapshot_download(
                    repo_id=self.repo_id,
                    repo_type="dataset",
                    local_dir=str(temp_dir),
                    token=self.token,
                    allow_patterns=[
                        "analysis-of-features/**",  # –°–∫–∞—á–∏–≤–∞–µ–º –ø–∞–ø–∫—É analysis-of-features
                        "workspace/excluded_features.txt"  # –°–∫–∞—á–∏–≤–∞–µ–º excluded_features.txt
                    ]
                )
                
                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                downloaded_path = Path(downloaded_path)
                analysis_source = downloaded_path / 'analysis-of-features'
                
                found_analysis = False
                if analysis_source.exists() and analysis_source.is_dir():
                    found_analysis = True
                    print(f"  –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏...")
                    for item in analysis_source.iterdir():
                        dest = local_path / item.name
                        if item.is_file():
                            if dest.exists():
                                dest.unlink()
                            shutil.move(str(item), str(dest))
                        else:
                            if dest.exists():
                                shutil.rmtree(dest)
                            shutil.move(str(item), str(dest))
                
                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º excluded_features.txt –≤ workspace, –µ—Å–ª–∏ –æ–Ω –Ω–∞–π–¥–µ–Ω
                excluded_source = downloaded_path / 'workspace' / 'excluded_features.txt'
                if excluded_source.exists():
                    workspace_path = Path('workspace')
                    workspace_path.mkdir(parents=True, exist_ok=True)
                    excluded_dest = workspace_path / 'excluded_features.txt'
                    if excluded_dest.exists():
                        excluded_dest.unlink()
                    shutil.move(str(excluded_source), str(excluded_dest))
                    print(f"  ‚úì –ü–µ—Ä–µ–º–µ—â–µ–Ω excluded_features.txt –≤ workspace/")
                
                if not found_analysis:
                    # –ü—Ä–æ–±—É–µ–º —Å—Ç–∞—Ä—ã–π –ø—É—Ç—å –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                    old_analysis_source = downloaded_path / 'features-analysis'
                    if old_analysis_source.exists() and old_analysis_source.is_dir():
                        found_analysis = True
                        print(f"  –ù–∞–π–¥–µ–Ω–∞ –ø–∞–ø–∫–∞ features-analysis (—Å—Ç–∞—Ä–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ), –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ...")
                        for item in old_analysis_source.iterdir():
                            dest = local_path / item.name
                            if item.is_file():
                                if dest.exists():
                                    dest.unlink()
                                shutil.move(str(item), str(dest))
                            else:
                                if dest.exists():
                                    shutil.rmtree(dest)
                                shutil.move(str(item), str(dest))
                
                if not found_analysis:
                    print("‚ö†Ô∏è  –ü–∞–ø–∫–∞ analysis-of-features –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏")
                    print(f"   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è. –û–∂–∏–¥–∞–µ—Ç—Å—è: analysis-of-features/")
                    return False
            finally:
                # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
                if temp_dir.exists():
                    shutil.rmtree(temp_dir, ignore_errors=True)
            
            print(f"\n‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω—ã!")
            print(f"  –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {local_dir}")
            print(f"\n  –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã:")
            for item in local_path.iterdir():
                if item.is_file():
                    size = item.stat().st_size
                    print(f"    - {item.name} ({format_size(size)})")
                elif item.is_dir():
                    size = get_directory_size(item)
                    print(f"    - {item.name}/ ({format_size(size)})")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    # –ê–ª–∏–∞—Å—ã –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    download_ticks = download_hf_ticks
    download_training_data = download_hf_training_data
    download_feature_analysis = download_hf_feature_analysis


def main():
    parser = argparse.ArgumentParser(
        description='–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –æ–±–ª–∞—á–Ω—ã–º–∏ —Å–µ—Ä–≤–∏—Å–∞–º–∏ (Hugging Face)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

Hugging Face:
  python cloud_services.py hf-upload-ticks --repo-id username/dataset-name
  python cloud_services.py hf-download-ticks --repo-id username/dataset-name
  python cloud_services.py hf-upload-training --repo-id username/dataset-name
  python cloud_services.py hf-download-training --repo-id username/dataset-name
  python cloud_services.py hf-upload-features --repo-id username/dataset-name
  python cloud_services.py hf-download-features --repo-id username/dataset-name
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='–ö–æ–º–∞–Ω–¥–∞')
    
    # Hugging Face: Upload ticks
    hf_upload_ticks_parser = subparsers.add_parser('hf-upload-ticks', help='–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ç–∏–∫–∏ –Ω–∞ Hugging Face')
    hf_upload_ticks_parser.add_argument('--repo-id', type=str, required=True, help='ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (username/dataset-name)')
    hf_upload_ticks_parser.add_argument('--token', type=str, default=None, help='Hugging Face —Ç–æ–∫–µ–Ω (–∏–ª–∏ HF_TOKEN env var)')
    hf_upload_ticks_parser.add_argument('--ticks-dir', type=str, default='workspace/raw_data/ticks', help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ç–∏–∫–∞–º–∏')
    hf_upload_ticks_parser.add_argument('--commit-message', type=str, default='Upload tick data', help='–°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞')
    
    # Hugging Face: Download ticks
    hf_download_ticks_parser = subparsers.add_parser('hf-download-ticks', help='–°–∫–∞—á–∞—Ç—å —Ç–∏–∫–∏ —Å Hugging Face')
    hf_download_ticks_parser.add_argument('--repo-id', type=str, required=True, help='ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (username/dataset-name)')
    hf_download_ticks_parser.add_argument('--token', type=str, default=None, help='Hugging Face —Ç–æ–∫–µ–Ω (–∏–ª–∏ HF_TOKEN env var)')
    hf_download_ticks_parser.add_argument('--local-dir', type=str, default='workspace/raw_data/ticks', help='–õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è')
    
    # Hugging Face: Upload training data
    hf_upload_training_parser = subparsers.add_parser('hf-upload-training', help='–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ Hugging Face')
    hf_upload_training_parser.add_argument('--repo-id', type=str, required=True, help='ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (username/dataset-name)')
    hf_upload_training_parser.add_argument('--token', type=str, default=None, help='Hugging Face —Ç–æ–∫–µ–Ω (–∏–ª–∏ HF_TOKEN env var)')
    hf_upload_training_parser.add_argument('--include-scalers', action='store_true', default=True, help='–í–∫–ª—é—á–∏—Ç—å scalers')
    hf_upload_training_parser.add_argument('--no-scalers', action='store_false', dest='include_scalers', help='–ù–µ –≤–∫–ª—é—á–∞—Ç—å scalers')
    hf_upload_training_parser.add_argument('--include-cache', action='store_true', help='–í–∫–ª—é—á–∏—Ç—å –∫—ç—à–∏')
    hf_upload_training_parser.add_argument('--commit-message', type=str, default='Upload training data', help='–°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞')
    
    # Hugging Face: Download training data
    hf_download_training_parser = subparsers.add_parser('hf-download-training', help='–°–∫–∞—á–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å Hugging Face')
    hf_download_training_parser.add_argument('--repo-id', type=str, required=True, help='ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (username/dataset-name)')
    hf_download_training_parser.add_argument('--token', type=str, default=None, help='Hugging Face —Ç–æ–∫–µ–Ω (–∏–ª–∏ HF_TOKEN env var)')
    hf_download_training_parser.add_argument('--local-dir', type=str, default='workspace', help='–õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è')
    
    # Hugging Face: Upload feature analysis
    hf_upload_features_parser = subparsers.add_parser('hf-upload-features', help='–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π –Ω–∞ Hugging Face')
    hf_upload_features_parser.add_argument('--repo-id', type=str, required=True, help='ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (username/dataset-name)')
    hf_upload_features_parser.add_argument('--token', type=str, default=None, help='Hugging Face —Ç–æ–∫–µ–Ω (–∏–ª–∏ HF_TOKEN env var)')
    hf_upload_features_parser.add_argument('--analysis-dir', type=str, default='workspace/analysis-of-features', help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞')
    hf_upload_features_parser.add_argument('--commit-message', type=str, default='Upload feature analysis results', help='–°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞')
    
    # Hugging Face: Download feature analysis
    hf_download_features_parser = subparsers.add_parser('hf-download-features', help='–°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π —Å Hugging Face')
    hf_download_features_parser.add_argument('--repo-id', type=str, required=True, help='ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (username/dataset-name)')
    hf_download_features_parser.add_argument('--token', type=str, default=None, help='Hugging Face —Ç–æ–∫–µ–Ω (–∏–ª–∏ HF_TOKEN env var)')
    hf_download_features_parser.add_argument('--local-dir', type=str, default='workspace/analysis-of-features', help='–õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è')
    
    # Hugging Face: Delete ticks
    hf_delete_ticks_parser = subparsers.add_parser('hf-delete-ticks', help='–£–¥–∞–ª–∏—Ç—å —Ç–∏–∫–∏ –∏–∑ Hugging Face –¥–∞—Ç–∞—Å–µ—Ç–∞')
    hf_delete_ticks_parser.add_argument('--repo-id', type=str, required=True, help='ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (username/dataset-name)')
    hf_delete_ticks_parser.add_argument('--token', type=str, default=None, help='Hugging Face —Ç–æ–∫–µ–Ω (–∏–ª–∏ HF_TOKEN env var)')
    hf_delete_ticks_parser.add_argument('--commit-message', type=str, default='Delete tick data', help='–°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞')
    
    # Hugging Face: Delete training data
    hf_delete_training_parser = subparsers.add_parser('hf-delete-training', help='–£–¥–∞–ª–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏–∑ Hugging Face –¥–∞—Ç–∞—Å–µ—Ç–∞')
    hf_delete_training_parser.add_argument('--repo-id', type=str, required=True, help='ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (username/dataset-name)')
    hf_delete_training_parser.add_argument('--token', type=str, default=None, help='Hugging Face —Ç–æ–∫–µ–Ω (–∏–ª–∏ HF_TOKEN env var)')
    hf_delete_training_parser.add_argument('--include-scalers', action='store_true', default=True, help='–£–¥–∞–ª—è—Ç—å scalers (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –¥–∞)')
    hf_delete_training_parser.add_argument('--no-scalers', action='store_false', dest='include_scalers', help='–ù–µ —É–¥–∞–ª—è—Ç—å scalers')
    hf_delete_training_parser.add_argument('--include-cache', action='store_true', default=True, help='–£–¥–∞–ª—è—Ç—å –∫—ç—à–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –¥–∞)')
    hf_delete_training_parser.add_argument('--no-cache', action='store_false', dest='include_cache', help='–ù–µ —É–¥–∞–ª—è—Ç—å –∫—ç—à–∏')
    hf_delete_training_parser.add_argument('--commit-message', type=str, default='Delete training data', help='–°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞')
    
    # Hugging Face: Delete feature analysis
    hf_delete_features_parser = subparsers.add_parser('hf-delete-features', help='–£–¥–∞–ª–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π –∏–∑ Hugging Face –¥–∞—Ç–∞—Å–µ—Ç–∞')
    hf_delete_features_parser.add_argument('--repo-id', type=str, required=True, help='ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (username/dataset-name)')
    hf_delete_features_parser.add_argument('--token', type=str, default=None, help='Hugging Face —Ç–æ–∫–µ–Ω (–∏–ª–∏ HF_TOKEN env var)')
    hf_delete_features_parser.add_argument('--commit-message', type=str, default='Delete feature analysis results', help='–°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞')
    
    # Hugging Face: Delete all data
    hf_delete_all_parser = subparsers.add_parser('hf-delete-all', help='–£–¥–∞–ª–∏—Ç—å –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ Hugging Face –¥–∞—Ç–∞—Å–µ—Ç–∞ (–æ—á–∏—Å—Ç–∏—Ç—å –¥–ª—è –Ω–æ–≤—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫)')
    hf_delete_all_parser.add_argument('--repo-id', type=str, required=True, help='ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (username/dataset-name)')
    hf_delete_all_parser.add_argument('--token', type=str, default=None, help='Hugging Face —Ç–æ–∫–µ–Ω (–∏–ª–∏ HF_TOKEN env var)')
    hf_delete_all_parser.add_argument('--commit-message', type=str, default='Delete all dataset data', help='–°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    if args.command == 'hf-upload-ticks':
        if not HF_AVAILABLE:
            print("‚ùå huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
            return
        uploader = HuggingFaceUploader(repo_id=args.repo_id, token=args.token)
        uploader.upload_hf_ticks(ticks_dir=args.ticks_dir, commit_message=args.commit_message)
    
    elif args.command == 'hf-download-ticks':
        if not HF_AVAILABLE:
            print("‚ùå huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
            return
        downloader = HuggingFaceDownloader(repo_id=args.repo_id, token=args.token)
        downloader.download_hf_ticks(local_dir=args.local_dir)
    
    elif args.command == 'hf-upload-training':
        if not HF_AVAILABLE:
            print("‚ùå huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
            return
        uploader = HuggingFaceUploader(repo_id=args.repo_id, token=args.token)
        uploader.upload_hf_training_data(
            include_scalers=args.include_scalers,
            include_cache=args.include_cache,
            commit_message=args.commit_message
        )
    
    elif args.command == 'hf-download-training':
        if not HF_AVAILABLE:
            print("‚ùå huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
            return
        downloader = HuggingFaceDownloader(repo_id=args.repo_id, token=args.token)
        downloader.download_hf_training_data(local_dir=args.local_dir)
    
    elif args.command == 'hf-upload-features':
        if not HF_AVAILABLE:
            print("‚ùå huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
            return
        uploader = HuggingFaceUploader(repo_id=args.repo_id, token=args.token)
        uploader.upload_hf_feature_analysis(
            analysis_dir=args.analysis_dir,
            commit_message=args.commit_message
        )
    
    elif args.command == 'hf-download-features':
        if not HF_AVAILABLE:
            print("‚ùå huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
            return
        downloader = HuggingFaceDownloader(repo_id=args.repo_id, token=args.token)
        downloader.download_hf_feature_analysis(local_dir=args.local_dir)
    
    elif args.command == 'hf-delete-ticks':
        if not HF_AVAILABLE:
            print("‚ùå huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
            return
        deleter = HuggingFaceDeleter(repo_id=args.repo_id, token=args.token)
        deleter.delete_hf_ticks(commit_message=args.commit_message)
    
    elif args.command == 'hf-delete-training':
        if not HF_AVAILABLE:
            print("‚ùå huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
            return
        deleter = HuggingFaceDeleter(repo_id=args.repo_id, token=args.token)
        include_scalers = getattr(args, 'include_scalers', True)
        include_cache = getattr(args, 'include_cache', True)
        deleter.delete_hf_training_data(
            include_scalers=include_scalers,
            include_cache=include_cache,
            commit_message=args.commit_message
        )
    
    elif args.command == 'hf-delete-features':
        if not HF_AVAILABLE:
            print("‚ùå huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
            return
        deleter = HuggingFaceDeleter(repo_id=args.repo_id, token=args.token)
        deleter.delete_hf_feature_analysis(commit_message=args.commit_message)
    
    elif args.command == 'hf-delete-all':
        if not HF_AVAILABLE:
            print("‚ùå huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
            return
        deleter = HuggingFaceDeleter(repo_id=args.repo_id, token=args.token)
        deleter.delete_all_data(commit_message=args.commit_message)
    
    print("\n" + "=" * 60)
    print("–ì–æ—Ç–æ–≤–æ!")
    print("=" * 60)


if __name__ == '__main__':
    main()

