"""
–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –æ–±–ª–∞—á–Ω—ã–º–∏ —Å–µ—Ä–≤–∏—Å–∞–º–∏ (Paperspace –∏ Hugging Face):
- –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
- –†–∞–±–æ—Ç–∞ —Å —Ç–∏–∫–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
- –ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π (--analyze-features)
"""
import os
import tarfile
import argparse
from pathlib import Path
from datetime import datetime
import subprocess
import sys
from typing import Optional, List
import shutil

try:
    from huggingface_hub import HfApi, upload_folder, snapshot_download
    HF_AVAILABLE = True
except ImportError:
    HF_AVAILABLE = False
    print("‚ö†Ô∏è  huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")


def get_directory_size(path: Path) -> int:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –≤ –±–∞–π—Ç–∞—Ö"""
    total = 0
    for dirpath, dirnames, filenames in os.walk(path):
        for filename in filenames:
            filepath = os.path.join(dirpath, filename)
            if os.path.exists(filepath):
                total += os.path.getsize(filepath)
    return total


def format_size(size_bytes: int) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–∞–∑–º–µ—Ä –≤ —á–∏—Ç–∞–µ–º—ã–π –≤–∏–¥"""
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if size_bytes < 1024.0:
            return f"{size_bytes:.2f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.2f} PB"


class PaperspaceUploader:
    """–ö–ª–∞—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ Paperspace –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, host: str = 'paperspace.com', path: str = '/storage/', user: Optional[str] = None):
        self.host = host
        self.path = path
        self.user = user
    
    def create_paperspace_training_archive(self, 
                               output_file: str,
                               include_ticks: bool = False,
                               include_cache: bool = False,
                               ask_ticks: bool = True) -> bool:
        """
        –°–æ–∑–¥–∞–µ—Ç tar.gz –∞—Ä—Ö–∏–≤ —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            output_file: –ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É
            include_ticks: –í–∫–ª—é—á–∞—Ç—å –ª–∏ —Ç–∏–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            include_cache: –í–∫–ª—é—á–∞—Ç—å –ª–∏ –∫—ç—à–∏
            ask_ticks: –°–ø—Ä–∞—à–∏–≤–∞—Ç—å –ª–∏ –æ –≤–∫–ª—é—á–µ–Ω–∏–∏ —Ç–∏–∫–æ–≤ (–µ—Å–ª–∏ include_ticks=True)
        """
        print("=" * 60)
        print("–£–ø–∞–∫–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ Paperspace")
        print("=" * 60)
        
        paths_to_include = []
        
        # CSV —Ñ–∞–π–ª—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)
        csv_files = [
            'workspace/prepared/features/gold_train.csv',
            'workspace/prepared/features/gold_val.csv',
            'workspace/prepared/features/gold_test.csv'
        ]
        
        for csv_file in csv_files:
            csv_path = Path(csv_file)
            if csv_path.exists():
                paths_to_include.append(csv_file)
                size = csv_path.stat().st_size
                print(f"‚úì –í–∫–ª—é—á–µ–Ω: {csv_file} ({format_size(size)})")
            else:
                print(f"‚ö† –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: {csv_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # –ö—ç—à–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if include_cache:
            cache_dir = Path('workspace/raw_data/cache')
            if cache_dir.exists():
                size = get_directory_size(cache_dir)
                paths_to_include.append(str(cache_dir))
                print(f"‚úì –í–∫–ª—é—á–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {cache_dir} ({format_size(size)})")
        
        # –¢–∏–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, —Å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ–º)
        if include_ticks:
            ticks_dir = Path('workspace/raw_data/ticks')
            if ticks_dir.exists():
                size = get_directory_size(ticks_dir)
                print(f"üìä –†–∞–∑–º–µ—Ä —Ç–∏–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {format_size(size)}")
                
                if ask_ticks:
                    response = input(f"–í–∫–ª—é—á–∏—Ç—å —Ç–∏–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ ({format_size(size)})? (y/n): ").strip().lower()
                    if response not in ['y', 'yes', '–¥–∞', '–¥', '']:
                        print("‚úó –¢–∏–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏—Å–∫–ª—é—á–µ–Ω—ã")
                        include_ticks = False
                
                if include_ticks:
                    paths_to_include.append(str(ticks_dir))
                    print(f"‚úì –í–∫–ª—é—á–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {ticks_dir}")
        
        if not paths_to_include:
            print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ø–∞–∫–æ–≤–∫–∏!")
            return False
        
        # –°–æ–∑–¥–∞–µ–º –∞—Ä—Ö–∏–≤
        print("\n" + "=" * 60)
        print("–°–æ–∑–¥–∞–Ω–∏–µ –∞—Ä—Ö–∏–≤–∞...")
        print("=" * 60)
        
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with tarfile.open(output_file, 'w:gz') as tar:
            for path_str in paths_to_include:
                path = Path(path_str)
                if path.exists():
                    print(f"–î–æ–±–∞–≤–ª–µ–Ω–∏–µ: {path_str}...")
                    tar.add(path_str, arcname=path_str, recursive=True)
                else:
                    print(f"‚ö† –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: {path_str} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
        
        archive_size = Path(output_file).stat().st_size
        print(f"\n‚úì –ê—Ä—Ö–∏–≤ —Å–æ–∑–¥–∞–Ω: {output_file}")
        print(f"  –†–∞–∑–º–µ—Ä: {format_size(archive_size)}")
        
        return True
    
    def upload_paperspace_training_data(self, archive_path: str, method: str = 'scp') -> bool:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∞—Ä—Ö–∏–≤ –Ω–∞ Paperspace
        
        Args:
            archive_path: –ü—É—Ç—å –∫ –∞—Ä—Ö–∏–≤—É
            method: –ú–µ—Ç–æ–¥ –∑–∞–≥—Ä—É–∑–∫–∏ ('scp' –∏–ª–∏ 'rsync')
        """
        if method == 'scp':
            return self._upload_via_scp(archive_path)
        elif method == 'rsync':
            return self._upload_via_rsync(archive_path)
        else:
            print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥: {method}")
            return False
    
    def _upload_via_scp(self, archive_path: str) -> bool:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∞—Ä—Ö–∏–≤ —á–µ—Ä–µ–∑ SCP"""
        print("\n" + "=" * 60)
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ Paperspace —á–µ—Ä–µ–∑ SCP")
        print("=" * 60)
        
        if self.user:
            scp_target = f"{self.user}@{self.host}:{self.path}"
        else:
            scp_target = f"{self.host}:{self.path}"
        
        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ {archive_path} -> {scp_target}")
        
        cmd = ['scp', archive_path, scp_target]
        print(f"–ö–æ–º–∞–Ω–¥–∞: {' '.join(cmd)}")
        
        try:
            subprocess.run(cmd, check=True)
            print("‚úì –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
            return True
        except subprocess.CalledProcessError as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
            return False
        except FileNotFoundError:
            print("‚ùå SCP –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ OpenSSH –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ rsync.")
            return False
    
    def _upload_via_rsync(self, archive_path: str) -> bool:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∞—Ä—Ö–∏–≤ —á–µ—Ä–µ–∑ RSYNC"""
        print("\n" + "=" * 60)
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ Paperspace —á–µ—Ä–µ–∑ RSYNC")
        print("=" * 60)
        
        if self.user:
            rsync_target = f"{self.user}@{self.host}:{self.path}"
        else:
            rsync_target = f"{self.host}:{self.path}"
        
        cmd = ['rsync', '-avz', '--progress', archive_path, rsync_target]
        print(f"–ö–æ–º–∞–Ω–¥–∞: {' '.join(cmd)}")
        
        try:
            subprocess.run(cmd, check=True)
            print("‚úì –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
            return True
        except subprocess.CalledProcessError as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
            return False
        except FileNotFoundError:
            print("‚ùå RSYNC –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ rsync.")
            return False
    
    # –ê–ª–∏–∞—Å—ã –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    create_training_archive = create_paperspace_training_archive
    upload_training_data = upload_paperspace_training_data


class PaperspaceDownloader:
    """–ö–ª–∞—Å—Å –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è —Å Paperspace"""
    
    def __init__(self, host: str = 'paperspace.com', path: str = '/storage/', user: Optional[str] = None):
        self.host = host
        self.path = path
        self.user = user
    
    def create_paperspace_results_archive(self, output_file: str) -> bool:
        """
        –°–æ–∑–¥–∞–µ—Ç tar.gz –∞—Ä—Ö–∏–≤ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è Paperspace
        
        Args:
            output_file: –ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É
        """
        print("=" * 60)
        print("–£–ø–∞–∫–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è")
        print("=" * 60)
        
        paths_to_include = []
        
        # –ú–æ–¥–µ–ª–∏
        models_dir = Path('workspace/models/checkpoints')
        if models_dir.exists():
            size = get_directory_size(models_dir)
            paths_to_include.append(str(models_dir))
            print(f"‚úì –í–∫–ª—é—á–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {models_dir} ({format_size(size)})")
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        metrics_dir = Path('workspace/models/metrics')
        if metrics_dir.exists():
            size = get_directory_size(metrics_dir)
            paths_to_include.append(str(metrics_dir))
            print(f"‚úì –í–∫–ª—é—á–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {metrics_dir} ({format_size(size)})")
        
        # Scalers
        scalers_dir = Path('workspace/prepared/scalers')
        if scalers_dir.exists():
            size = get_directory_size(scalers_dir)
            paths_to_include.append(str(scalers_dir))
            print(f"‚úì –í–∫–ª—é—á–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {scalers_dir} ({format_size(size)})")
        
        # TensorBoard –ª–æ–≥–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        logs_dir = Path('workspace/models/logs')
        if logs_dir.exists():
            size = get_directory_size(logs_dir)
            paths_to_include.append(str(logs_dir))
            print(f"‚úì –í–∫–ª—é—á–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {logs_dir} ({format_size(size)})")
        
        if not paths_to_include:
            print("‚ùå –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —É–ø–∞–∫–æ–≤–∫–∏!")
            return False
        
        # –°–æ–∑–¥–∞–µ–º –∞—Ä—Ö–∏–≤
        print("\n" + "=" * 60)
        print("–°–æ–∑–¥–∞–Ω–∏–µ –∞—Ä—Ö–∏–≤–∞...")
        print("=" * 60)
        
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with tarfile.open(output_file, 'w:gz') as tar:
            for path_str in paths_to_include:
                path = Path(path_str)
                if path.exists():
                    print(f"–î–æ–±–∞–≤–ª–µ–Ω–∏–µ: {path_str}...")
                    tar.add(path_str, arcname=path_str, recursive=True)
        
        archive_size = Path(output_file).stat().st_size
        print(f"\n‚úì –ê—Ä—Ö–∏–≤ —Å–æ–∑–¥–∞–Ω: {output_file}")
        print(f"  –†–∞–∑–º–µ—Ä: {format_size(archive_size)}")
        
        return True
    
    def download_paperspace_results(self, remote_archive: str, local_path: str = '.', method: str = 'scp') -> bool:
        """
        –°–∫–∞—á–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è —Å Paperspace
        
        Args:
            remote_archive: –ü—É—Ç—å –∫ –∞—Ä—Ö–∏–≤—É –Ω–∞ Paperspace
            local_path: –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            method: –ú–µ—Ç–æ–¥ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è ('scp' –∏–ª–∏ 'rsync')
        """
        if method == 'scp':
            return self._download_via_scp(remote_archive, local_path)
        elif method == 'rsync':
            return self._download_via_rsync(remote_archive, local_path)
        else:
            print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥: {method}")
            return False
    
    def _download_via_scp(self, remote_archive: str, local_path: str) -> bool:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –∞—Ä—Ö–∏–≤ —á–µ—Ä–µ–∑ SCP"""
        print("\n" + "=" * 60)
        print("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ SCP")
        print("=" * 60)
        
        if self.user:
            scp_source = f"{self.user}@{self.host}:{remote_archive}"
        else:
            scp_source = f"{self.host}:{remote_archive}"
        
        local_file = Path(local_path) / Path(remote_archive).name
        print(f"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ {scp_source} -> {local_file}")
        
        cmd = ['scp', scp_source, str(local_file)]
        print(f"–ö–æ–º–∞–Ω–¥–∞: {' '.join(cmd)}")
        
        try:
            subprocess.run(cmd, check=True)
            print("‚úì –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
            print(f"  –†–∞—Å–ø–∞–∫—É–π—Ç–µ –∞—Ä—Ö–∏–≤: tar -xzf {local_file.name}")
            return True
        except subprocess.CalledProcessError as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏: {e}")
            return False
        except FileNotFoundError:
            print("‚ùå SCP –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ OpenSSH.")
            return False
    
    def _download_via_rsync(self, remote_archive: str, local_path: str) -> bool:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –∞—Ä—Ö–∏–≤ —á–µ—Ä–µ–∑ RSYNC"""
        print("\n" + "=" * 60)
        print("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ RSYNC")
        print("=" * 60)
        
        if self.user:
            rsync_source = f"{self.user}@{self.host}:{remote_archive}"
        else:
            rsync_source = f"{self.host}:{remote_archive}"
        
        local_file = Path(local_path) / Path(remote_archive).name
        cmd = ['rsync', '-avz', '--progress', rsync_source, str(local_file)]
        print(f"–ö–æ–º–∞–Ω–¥–∞: {' '.join(cmd)}")
        
        try:
            subprocess.run(cmd, check=True)
            print("‚úì –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
            print(f"  –†–∞—Å–ø–∞–∫—É–π—Ç–µ –∞—Ä—Ö–∏–≤: tar -xzf {local_file.name}")
            return True
        except subprocess.CalledProcessError as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏: {e}")
            return False
        except FileNotFoundError:
            print("‚ùå RSYNC –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ rsync.")
            return False
    
    def list_paperspace_files(self, remote_path: str = None) -> bool:
        """
        –í—ã–≤–æ–¥–∏—Ç —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –Ω–∞ Paperspace
        
        Args:
            remote_path: –ü—É—Ç—å –Ω–∞ —É–¥–∞–ª–µ–Ω–Ω–æ–º —Å–µ—Ä–≤–µ—Ä–µ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é self.path)
        """
        if remote_path is None:
            remote_path = self.path
        
        print("\n" + "=" * 60)
        print("–°–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –Ω–∞ Paperspace")
        print("=" * 60)
        
        if self.user:
            ssh_target = f"{self.user}@{self.host}"
        else:
            ssh_target = self.host
        
        cmd = ['ssh', ssh_target, f'ls -lh {remote_path}']
        print(f"–ö–æ–º–∞–Ω–¥–∞: {' '.join(cmd)}")
        
        try:
            subprocess.run(cmd, check=True)
            return True
        except subprocess.CalledProcessError as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤: {e}")
            return False
        except FileNotFoundError:
            print("‚ùå SSH –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ OpenSSH.")
            return False
    
    # –ê–ª–∏–∞—Å—ã –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    create_results_archive = create_paperspace_results_archive
    download_results = download_paperspace_results
    list_remote_files = list_paperspace_files


class HuggingFaceUploader:
    """–ö–ª–∞—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ Hugging Face Hub"""
    
    def __init__(self, repo_id: str, token: Optional[str] = None):
        """
        Args:
            repo_id: ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –Ω–∞ Hugging Face (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'username/dataset-name')
            token: Hugging Face —Ç–æ–∫–µ–Ω (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è HF_TOKEN)
        """
        if not HF_AVAILABLE:
            raise ImportError("huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
        
        self.repo_id = repo_id
        self.api = HfApi(token=token)
        self.token = token or os.getenv('HF_TOKEN')
        
        if not self.token:
            print("‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: HF_TOKEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ú–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è.")
    
    def upload_hf_ticks(self, ticks_dir: str = 'workspace/raw_data/ticks', 
                     commit_message: str = "Upload tick data") -> bool:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–∏–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ Hugging Face
        
        Args:
            ticks_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ç–∏–∫–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            commit_message: –°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞
        """
        print("=" * 60)
        print("–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–∏–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ Hugging Face")
        print("=" * 60)
        
        ticks_path = Path(ticks_dir)
        if not ticks_path.exists():
            print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {ticks_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
            return False
        
        size = get_directory_size(ticks_path)
        print(f"üìä –†–∞–∑–º–µ—Ä —Ç–∏–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {format_size(size)}")
        print(f"üìÅ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {self.repo_id}")
        print(f"üìÇ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {ticks_dir}")
        
        try:
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            temp_dir = Path('temp_hf_upload')
            temp_ticks_dir = temp_dir / 'ticks'
            temp_ticks_dir.mkdir(parents=True, exist_ok=True)
            
            # –ö–æ–ø–∏—Ä—É–µ–º —Ç–∏–∫–∏ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            print(f"\n–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
            shutil.copytree(ticks_path, temp_ticks_dir, dirs_exist_ok=True)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞ Hugging Face
            print(f"\n–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ Hugging Face...")
            upload_folder(
                folder_path=str(temp_dir),
                repo_id=self.repo_id,
                repo_type="dataset",
                token=self.token,
                commit_message=commit_message
            )
            
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            shutil.rmtree(temp_dir, ignore_errors=True)
            
            print(f"\n‚úì –¢–∏–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –Ω–∞ Hugging Face!")
            print(f"  –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: https://huggingface.co/datasets/{self.repo_id}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
            shutil.rmtree(temp_dir, ignore_errors=True)
            return False
    
    def upload_hf_feature_analysis(self, 
                               analysis_dir: str = 'workspace/features-analysis',
                               commit_message: str = "Upload feature analysis results") -> bool:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π (--analyze-features) –Ω–∞ Hugging Face
        
        Args:
            analysis_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: workspace/features-analysis)
            commit_message: –°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞
        """
        print("=" * 60)
        print("–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π –Ω–∞ Hugging Face")
        print("=" * 60)
        
        analysis_path = Path(analysis_dir)
        if not analysis_path.exists():
            print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {analysis_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
            print("   –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python full_pipeline.py --analyze-features")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        expected_files = [
            'feature_statistics.csv',
            'feature_importance.csv',
            'outliers_analysis.csv',
            'feature_by_class_statistics.csv',
            'feature_analysis_report.html'
        ]
        
        found_files = []
        for file_name in expected_files:
            file_path = analysis_path / file_name
            if file_path.exists():
                found_files.append(file_name)
                size = file_path.stat().st_size
                print(f"‚úì –ù–∞–π–¥–µ–Ω: {file_name} ({format_size(size)})")
            else:
                print(f"‚ö† –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç: {file_name}")
        
        if not found_files:
            print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞!")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ plots (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        plots_dir = analysis_path / 'plots'
        has_plots = plots_dir.exists() and plots_dir.is_dir()
        if has_plots:
            plots_size = get_directory_size(plots_dir)
            print(f"‚úì –ù–∞–π–¥–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è plots ({format_size(plots_size)})")
        
        size = get_directory_size(analysis_path)
        print(f"\nüìä –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {format_size(size)}")
        print(f"üìÅ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {self.repo_id}")
        print(f"üìÇ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {analysis_dir}")
        
        try:
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            temp_dir = Path('temp_hf_upload')
            temp_analysis_dir = temp_dir / 'features-analysis'
            temp_analysis_dir.mkdir(parents=True, exist_ok=True)
            
            # –ö–æ–ø–∏—Ä—É–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            print(f"\n–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
            for item in analysis_path.iterdir():
                if item.is_file():
                    shutil.copy2(item, temp_analysis_dir / item.name)
                    print(f"  –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω —Ñ–∞–π–ª: {item.name}")
                elif item.is_dir():
                    shutil.copytree(item, temp_analysis_dir / item.name, dirs_exist_ok=True)
                    print(f"  –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {item.name}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞ Hugging Face
            print(f"\n–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ Hugging Face...")
            upload_folder(
                folder_path=str(temp_dir),
                repo_id=self.repo_id,
                repo_type="dataset",
                token=self.token,
                commit_message=commit_message
            )
            
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            shutil.rmtree(temp_dir, ignore_errors=True)
            
            print(f"\n‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –Ω–∞ Hugging Face!")
            print(f"  –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: https://huggingface.co/datasets/{self.repo_id}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
            import traceback
            traceback.print_exc()
            shutil.rmtree(temp_dir, ignore_errors=True)
            return False
    
    def upload_hf_training_data(self,
                             include_scalers: bool = True,
                             include_cache: bool = False,
                             commit_message: str = "Upload training data") -> bool:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ Hugging Face (–±–µ–∑ —Ç–∏–∫–æ–≤)
        
        Args:
            include_scalers: –í–∫–ª—é—á–∞—Ç—å –ª–∏ scalers
            include_cache: –í–∫–ª—é—á–∞—Ç—å –ª–∏ –∫—ç—à–∏
            commit_message: –°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞
        """
        print("=" * 60)
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ Hugging Face")
        print("=" * 60)
        
        paths_to_include = []
        
        # CSV —Ñ–∞–π–ª—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)
        csv_files = [
            'workspace/prepared/features/gold_train.csv',
            'workspace/prepared/features/gold_val.csv',
            'workspace/prepared/features/gold_test.csv'
        ]
        
        for csv_file in csv_files:
            csv_path = Path(csv_file)
            if csv_path.exists():
                paths_to_include.append(csv_file)
                size = csv_path.stat().st_size
                print(f"‚úì –í–∫–ª—é—á–µ–Ω: {csv_file} ({format_size(size)})")
            else:
                print(f"‚ö† –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: {csv_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # Scalers (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if include_scalers:
            scalers_dir = Path('workspace/prepared/scalers')
            if scalers_dir.exists():
                size = get_directory_size(scalers_dir)
                paths_to_include.append(str(scalers_dir))
                print(f"‚úì –í–∫–ª—é—á–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {scalers_dir} ({format_size(size)})")
        
        # –ö—ç—à–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if include_cache:
            cache_dir = Path('workspace/raw_data/cache')
            if cache_dir.exists():
                size = get_directory_size(cache_dir)
                paths_to_include.append(str(cache_dir))
                print(f"‚úì –í–∫–ª—é—á–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {cache_dir} ({format_size(size)})")
        
        if not paths_to_include:
            print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏!")
            return False
        
        print(f"\nüìÅ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {self.repo_id}")
        
        try:
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            temp_dir = Path('temp_hf_upload')
            temp_dir.mkdir(parents=True, exist_ok=True)
            
            # –ö–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            print(f"\n–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
            for path_str in paths_to_include:
                path = Path(path_str)
                if path.exists():
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
                    rel_path = path.relative_to(Path('workspace').parent)
                    dest_path = temp_dir / rel_path
                    dest_path.parent.mkdir(parents=True, exist_ok=True)
                    
                    if path.is_file():
                        shutil.copy2(path, dest_path)
                    else:
                        shutil.copytree(path, dest_path, dirs_exist_ok=True)
                    print(f"  –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ: {path_str}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞ Hugging Face
            print(f"\n–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ Hugging Face...")
            upload_folder(
                folder_path=str(temp_dir),
                repo_id=self.repo_id,
                repo_type="dataset",
                token=self.token,
                commit_message=commit_message
            )
            
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            shutil.rmtree(temp_dir, ignore_errors=True)
            
            print(f"\n‚úì –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –Ω–∞ Hugging Face!")
            print(f"  –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: https://huggingface.co/datasets/{self.repo_id}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
            import traceback
            traceback.print_exc()
            shutil.rmtree(temp_dir, ignore_errors=True)
            return False
    
    # –ê–ª–∏–∞—Å—ã –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    upload_ticks = upload_hf_ticks
    upload_feature_analysis = upload_hf_feature_analysis
    upload_training_data = upload_hf_training_data


class HuggingFaceDownloader:
    """–ö–ª–∞—Å—Å –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —Å Hugging Face Hub"""
    
    def __init__(self, repo_id: str, token: Optional[str] = None):
        """
        Args:
            repo_id: ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –Ω–∞ Hugging Face (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'username/dataset-name')
            token: Hugging Face —Ç–æ–∫–µ–Ω (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è HF_TOKEN)
        """
        if not HF_AVAILABLE:
            raise ImportError("huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
        
        self.repo_id = repo_id
        self.api = HfApi(token=token)
        self.token = token or os.getenv('HF_TOKEN')
    
    def download_hf_ticks(self, local_dir: str = 'workspace/raw_data/ticks') -> bool:
        """
        –°–∫–∞—á–∏–≤–∞–µ—Ç —Ç–∏–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å Hugging Face
        
        Args:
            local_dir: –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        print("=" * 60)
        print("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ç–∏–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å Hugging Face")
        print("=" * 60)
        
        print(f"üìÅ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {self.repo_id}")
        print(f"üìÇ –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {local_dir}")
        
        try:
            local_path = Path(local_dir)
            local_path.mkdir(parents=True, exist_ok=True)
            
            # –°–∫–∞—á–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            print(f"\n–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
            downloaded_path = snapshot_download(
                repo_id=self.repo_id,
                repo_type="dataset",
                local_dir=str(local_path),
                token=self.token
            )
            
            # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ —Å–∫–∞—á–∞–ª–∏—Å—å –≤ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é ticks, –ø–µ—Ä–µ–º–µ—â–∞–µ–º –∏—Ö
            downloaded_path = Path(downloaded_path)
            ticks_subdir = downloaded_path / 'ticks'
            if ticks_subdir.exists() and ticks_subdir.is_dir():
                # –î–∞–Ω–Ω—ã–µ –≤ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ ticks, –ø–µ—Ä–µ–º–µ—â–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ (–Ω–µ –∫–æ–ø–∏—Ä—É–µ–º!)
                print(f"  –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏...")
                for item in ticks_subdir.iterdir():
                    dest = local_path / item.name
                    if item.is_file():
                        if dest.exists():
                            dest.unlink()  # –£–¥–∞–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª
                        shutil.move(str(item), str(dest))  # –ü–µ—Ä–µ–º–µ—â–∞–µ–º
                    else:
                        if dest.exists():
                            shutil.rmtree(dest)
                        shutil.move(str(item), str(dest))  # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
                
                # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—É—é –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é ticks –ø–æ—Å–ª–µ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è
                try:
                    ticks_subdir.rmdir()  # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
                except OSError:
                    # –ï—Å–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –ø—É—Å—Ç–∞—è, —É–¥–∞–ª—è–µ–º —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ
                    shutil.rmtree(ticks_subdir)
            
            print(f"\n‚úì –¢–∏–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω—ã!")
            print(f"  –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {local_dir}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def download_hf_training_data(self, local_dir: str = 'workspace') -> bool:
        """
        –°–∫–∞—á–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å Hugging Face
        
        Args:
            local_dir: –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        print("=" * 60)
        print("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å Hugging Face")
        print("=" * 60)
        
        print(f"üìÅ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {self.repo_id}")
        print(f"üìÇ –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {local_dir}")
        
        try:
            local_path = Path(local_dir)
            local_path.mkdir(parents=True, exist_ok=True)
            
            # –°–∫–∞—á–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            print(f"\n–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
            snapshot_download(
                repo_id=self.repo_id,
                repo_type="dataset",
                local_dir=str(local_path),
                token=self.token
            )
            
            print(f"\n‚úì –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω—ã!")
            print(f"  –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {local_dir}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def download_hf_feature_analysis(self, local_dir: str = 'workspace/features-analysis') -> bool:
        """
        –°–∫–∞—á–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π —Å Hugging Face
        
        Args:
            local_dir: –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: workspace/features-analysis)
        """
        print("=" * 60)
        print("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π —Å Hugging Face")
        print("=" * 60)
        
        print(f"üìÅ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {self.repo_id}")
        print(f"üìÇ –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {local_dir}")
        
        try:
            local_path = Path(local_dir)
            local_path.mkdir(parents=True, exist_ok=True)
            
            # –°–∫–∞—á–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            print(f"\n–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
            downloaded_path = snapshot_download(
                repo_id=self.repo_id,
                repo_type="dataset",
                local_dir=str(local_path.parent),
                token=self.token
            )
            
            # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ —Å–∫–∞—á–∞–ª–∏—Å—å –≤ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é features-analysis, –ø–µ—Ä–µ–º–µ—â–∞–µ–º –∏—Ö
            downloaded_path = Path(downloaded_path)
            analysis_subdir = downloaded_path / 'features-analysis'
            if analysis_subdir.exists() and analysis_subdir.is_dir():
                # –î–∞–Ω–Ω—ã–µ –≤ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ features-analysis, –ø–µ—Ä–µ–º–µ—â–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                print(f"  –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏...")
                for item in analysis_subdir.iterdir():
                    dest = local_path / item.name
                    if item.is_file():
                        if dest.exists():
                            dest.unlink()  # –£–¥–∞–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª
                        shutil.move(str(item), str(dest))  # –ü–µ—Ä–µ–º–µ—â–∞–µ–º
                    else:
                        if dest.exists():
                            shutil.rmtree(dest)
                        shutil.move(str(item), str(dest))  # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
                
                # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—É—é –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é features-analysis –ø–æ—Å–ª–µ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è
                try:
                    analysis_subdir.rmdir()  # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
                except OSError:
                    # –ï—Å–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –ø—É—Å—Ç–∞—è, —É–¥–∞–ª—è–µ–º —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ
                    shutil.rmtree(analysis_subdir)
            
            print(f"\n‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω—ã!")
            print(f"  –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {local_dir}")
            print(f"\n  –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã:")
            for item in local_path.iterdir():
                if item.is_file():
                    size = item.stat().st_size
                    print(f"    - {item.name} ({format_size(size)})")
                elif item.is_dir():
                    size = get_directory_size(item)
                    print(f"    - {item.name}/ ({format_size(size)})")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    # –ê–ª–∏–∞—Å—ã –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    download_ticks = download_hf_ticks
    download_training_data = download_hf_training_data
    download_feature_analysis = download_hf_feature_analysis


def main():
    parser = argparse.ArgumentParser(
        description='–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –æ–±–ª–∞—á–Ω—ã–º–∏ —Å–µ—Ä–≤–∏—Å–∞–º–∏ (Paperspace –∏ Hugging Face)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

Paperspace:
  python cloud_services.py upload-training --host paperspace.com --path /storage/
  python cloud_services.py create-training-archive --output training_data.tar.gz
  python cloud_services.py download-results --host paperspace.com --path /storage/results.tar.gz
  python cloud_services.py create-results-archive --output results.tar.gz
  python cloud_services.py list-remote-files --host paperspace.com --path /storage/

Hugging Face:
  python cloud_services.py hf-upload-ticks --repo-id username/dataset-name
  python cloud_services.py hf-download-ticks --repo-id username/dataset-name
  python cloud_services.py hf-upload-training --repo-id username/dataset-name
  python cloud_services.py hf-download-training --repo-id username/dataset-name
  python cloud_services.py hf-upload-features --repo-id username/dataset-name
  python cloud_services.py hf-download-features --repo-id username/dataset-name
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='–ö–æ–º–∞–Ω–¥–∞')
    
    # Upload training data
    upload_parser = subparsers.add_parser('upload-training', help='–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è')
    upload_parser.add_argument('--host', type=str, default='paperspace.com', help='–•–æ—Å—Ç Paperspace')
    upload_parser.add_argument('--path', type=str, default='/storage/', help='–ü—É—Ç—å –Ω–∞ Paperspace')
    upload_parser.add_argument('--user', type=str, default=None, help='–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å')
    upload_parser.add_argument('--method', type=str, choices=['scp', 'rsync'], default='scp', help='–ú–µ—Ç–æ–¥ –∑–∞–≥—Ä—É–∑–∫–∏')
    upload_parser.add_argument('--include-ticks', action='store_true', help='–í–∫–ª—é—á–∏—Ç—å —Ç–∏–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ')
    upload_parser.add_argument('--include-cache', action='store_true', help='–í–∫–ª—é—á–∏—Ç—å –∫—ç—à–∏')
    upload_parser.add_argument('--no-ask-ticks', action='store_true', help='–ù–µ —Å–ø—Ä–∞—à–∏–≤–∞—Ç—å –æ —Ç–∏–∫–∞—Ö')
    
    # Create training archive
    create_training_parser = subparsers.add_parser('create-training-archive', help='–°–æ–∑–¥–∞—Ç—å –∞—Ä—Ö–∏–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è')
    create_training_parser.add_argument('--output', '-o', type=str,
                                      default=f'training_data_{datetime.now().strftime("%Y%m%d_%H%M%S")}.tar.gz',
                                      help='–ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É –∞—Ä—Ö–∏–≤—É')
    create_training_parser.add_argument('--include-ticks', action='store_true', help='–í–∫–ª—é—á–∏—Ç—å —Ç–∏–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ')
    create_training_parser.add_argument('--include-cache', action='store_true', help='–í–∫–ª—é—á–∏—Ç—å –∫—ç—à–∏')
    create_training_parser.add_argument('--no-ask-ticks', action='store_true', help='–ù–µ —Å–ø—Ä–∞—à–∏–≤–∞—Ç—å –æ —Ç–∏–∫–∞—Ö')
    
    # Download results
    download_parser = subparsers.add_parser('download-results', help='–°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è')
    download_parser.add_argument('--host', type=str, default='paperspace.com', help='–•–æ—Å—Ç Paperspace')
    download_parser.add_argument('--path', type=str, required=True, help='–ü—É—Ç—å –∫ –∞—Ä—Ö–∏–≤—É –Ω–∞ Paperspace')
    download_parser.add_argument('--user', type=str, default=None, help='–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å')
    download_parser.add_argument('--method', type=str, choices=['scp', 'rsync'], default='scp', help='–ú–µ—Ç–æ–¥ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è')
    download_parser.add_argument('--local-path', type=str, default='.', help='–õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è')
    
    # Create results archive
    create_results_parser = subparsers.add_parser('create-results-archive', help='–°–æ–∑–¥–∞—Ç—å –∞—Ä—Ö–∏–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
    create_results_parser.add_argument('--output', '-o', type=str,
                                     default=f'results_{datetime.now().strftime("%Y%m%d_%H%M%S")}.tar.gz',
                                     help='–ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É –∞—Ä—Ö–∏–≤—É')
    
    # List remote files
    list_parser = subparsers.add_parser('list-remote-files', help='–°–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –Ω–∞ Paperspace')
    list_parser.add_argument('--host', type=str, default='paperspace.com', help='–•–æ—Å—Ç Paperspace')
    list_parser.add_argument('--path', type=str, default='/storage/', help='–ü—É—Ç—å –Ω–∞ Paperspace')
    list_parser.add_argument('--user', type=str, default=None, help='–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å')
    
    # Hugging Face: Upload ticks
    hf_upload_ticks_parser = subparsers.add_parser('hf-upload-ticks', help='–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ç–∏–∫–∏ –Ω–∞ Hugging Face')
    hf_upload_ticks_parser.add_argument('--repo-id', type=str, required=True, help='ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (username/dataset-name)')
    hf_upload_ticks_parser.add_argument('--token', type=str, default=None, help='Hugging Face —Ç–æ–∫–µ–Ω (–∏–ª–∏ HF_TOKEN env var)')
    hf_upload_ticks_parser.add_argument('--ticks-dir', type=str, default='workspace/raw_data/ticks', help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ç–∏–∫–∞–º–∏')
    hf_upload_ticks_parser.add_argument('--commit-message', type=str, default='Upload tick data', help='–°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞')
    
    # Hugging Face: Download ticks
    hf_download_ticks_parser = subparsers.add_parser('hf-download-ticks', help='–°–∫–∞—á–∞—Ç—å —Ç–∏–∫–∏ —Å Hugging Face')
    hf_download_ticks_parser.add_argument('--repo-id', type=str, required=True, help='ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (username/dataset-name)')
    hf_download_ticks_parser.add_argument('--token', type=str, default=None, help='Hugging Face —Ç–æ–∫–µ–Ω (–∏–ª–∏ HF_TOKEN env var)')
    hf_download_ticks_parser.add_argument('--local-dir', type=str, default='workspace/raw_data/ticks', help='–õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è')
    
    # Hugging Face: Upload training data
    hf_upload_training_parser = subparsers.add_parser('hf-upload-training', help='–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ Hugging Face')
    hf_upload_training_parser.add_argument('--repo-id', type=str, required=True, help='ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (username/dataset-name)')
    hf_upload_training_parser.add_argument('--token', type=str, default=None, help='Hugging Face —Ç–æ–∫–µ–Ω (–∏–ª–∏ HF_TOKEN env var)')
    hf_upload_training_parser.add_argument('--include-scalers', action='store_true', default=True, help='–í–∫–ª—é—á–∏—Ç—å scalers')
    hf_upload_training_parser.add_argument('--no-scalers', action='store_false', dest='include_scalers', help='–ù–µ –≤–∫–ª—é—á–∞—Ç—å scalers')
    hf_upload_training_parser.add_argument('--include-cache', action='store_true', help='–í–∫–ª—é—á–∏—Ç—å –∫—ç—à–∏')
    hf_upload_training_parser.add_argument('--commit-message', type=str, default='Upload training data', help='–°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞')
    
    # Hugging Face: Download training data
    hf_download_training_parser = subparsers.add_parser('hf-download-training', help='–°–∫–∞—á–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å Hugging Face')
    hf_download_training_parser.add_argument('--repo-id', type=str, required=True, help='ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (username/dataset-name)')
    hf_download_training_parser.add_argument('--token', type=str, default=None, help='Hugging Face —Ç–æ–∫–µ–Ω (–∏–ª–∏ HF_TOKEN env var)')
    hf_download_training_parser.add_argument('--local-dir', type=str, default='workspace', help='–õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è')
    
    # Hugging Face: Upload feature analysis
    hf_upload_features_parser = subparsers.add_parser('hf-upload-features', help='–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π –Ω–∞ Hugging Face')
    hf_upload_features_parser.add_argument('--repo-id', type=str, required=True, help='ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (username/dataset-name)')
    hf_upload_features_parser.add_argument('--token', type=str, default=None, help='Hugging Face —Ç–æ–∫–µ–Ω (–∏–ª–∏ HF_TOKEN env var)')
    hf_upload_features_parser.add_argument('--analysis-dir', type=str, default='workspace/features-analysis', help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞')
    hf_upload_features_parser.add_argument('--commit-message', type=str, default='Upload feature analysis results', help='–°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞')
    
    # Hugging Face: Download feature analysis
    hf_download_features_parser = subparsers.add_parser('hf-download-features', help='–°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π —Å Hugging Face')
    hf_download_features_parser.add_argument('--repo-id', type=str, required=True, help='ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (username/dataset-name)')
    hf_download_features_parser.add_argument('--token', type=str, default=None, help='Hugging Face —Ç–æ–∫–µ–Ω (–∏–ª–∏ HF_TOKEN env var)')
    hf_download_features_parser.add_argument('--local-dir', type=str, default='workspace/features-analysis', help='–õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    if args.command == 'upload-training':
        uploader = PaperspaceUploader(host=args.host, path=args.path, user=args.user)
        archive_name = f'training_data_{datetime.now().strftime("%Y%m%d_%H%M%S")}.tar.gz'
        if uploader.create_paperspace_training_archive(archive_name,
                                           include_ticks=args.include_ticks,
                                           include_cache=args.include_cache,
                                           ask_ticks=not args.no_ask_ticks):
            uploader.upload_paperspace_training_data(archive_name, method=args.method)
    
    elif args.command == 'create-training-archive':
        uploader = PaperspaceUploader()
        uploader.create_paperspace_training_archive(
            output_file=args.output,
            include_ticks=args.include_ticks,
            include_cache=args.include_cache,
            ask_ticks=not args.no_ask_ticks
        )
    
    elif args.command == 'download-results':
        downloader = PaperspaceDownloader(host=args.host, user=args.user)
        downloader.download_paperspace_results(args.path, local_path=args.local_path, method=args.method)
    
    elif args.command == 'create-results-archive':
        downloader = PaperspaceDownloader()
        downloader.create_paperspace_results_archive(args.output)
    
    elif args.command == 'list-remote-files':
        downloader = PaperspaceDownloader(host=args.host, path=args.path, user=args.user)
        downloader.list_paperspace_files()
    
    elif args.command == 'hf-upload-ticks':
        if not HF_AVAILABLE:
            print("‚ùå huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
            return
        uploader = HuggingFaceUploader(repo_id=args.repo_id, token=args.token)
        uploader.upload_hf_ticks(ticks_dir=args.ticks_dir, commit_message=args.commit_message)
    
    elif args.command == 'hf-download-ticks':
        if not HF_AVAILABLE:
            print("‚ùå huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
            return
        downloader = HuggingFaceDownloader(repo_id=args.repo_id, token=args.token)
        downloader.download_hf_ticks(local_dir=args.local_dir)
    
    elif args.command == 'hf-upload-training':
        if not HF_AVAILABLE:
            print("‚ùå huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
            return
        uploader = HuggingFaceUploader(repo_id=args.repo_id, token=args.token)
        uploader.upload_hf_training_data(
            include_scalers=args.include_scalers,
            include_cache=args.include_cache,
            commit_message=args.commit_message
        )
    
    elif args.command == 'hf-download-training':
        if not HF_AVAILABLE:
            print("‚ùå huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
            return
        downloader = HuggingFaceDownloader(repo_id=args.repo_id, token=args.token)
        downloader.download_hf_training_data(local_dir=args.local_dir)
    
    elif args.command == 'hf-upload-features':
        if not HF_AVAILABLE:
            print("‚ùå huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
            return
        uploader = HuggingFaceUploader(repo_id=args.repo_id, token=args.token)
        uploader.upload_hf_feature_analysis(
            analysis_dir=args.analysis_dir,
            commit_message=args.commit_message
        )
    
    elif args.command == 'hf-download-features':
        if not HF_AVAILABLE:
            print("‚ùå huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
            return
        downloader = HuggingFaceDownloader(repo_id=args.repo_id, token=args.token)
        downloader.download_hf_feature_analysis(local_dir=args.local_dir)
    
    print("\n" + "=" * 60)
    print("–ì–æ—Ç–æ–≤–æ!")
    print("=" * 60)


if __name__ == '__main__':
    main()

