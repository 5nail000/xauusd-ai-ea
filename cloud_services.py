"""
–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –æ–±–ª–∞—á–Ω—ã–º–∏ —Å–µ—Ä–≤–∏—Å–∞–º–∏ (Paperspace –∏ Hugging Face):
- –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
- –†–∞–±–æ—Ç–∞ —Å —Ç–∏–∫–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
- –ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π (--analyze-features)
"""
import os
import tarfile
import argparse
from pathlib import Path
from datetime import datetime
import subprocess
import sys
from typing import Optional, List
import shutil

try:
    from huggingface_hub import HfApi, upload_folder, snapshot_download
    HF_AVAILABLE = True
except ImportError:
    HF_AVAILABLE = False
    print("‚ö†Ô∏è  huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")


def get_directory_size(path: Path) -> int:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –≤ –±–∞–π—Ç–∞—Ö"""
    total = 0
    for dirpath, dirnames, filenames in os.walk(path):
        for filename in filenames:
            filepath = os.path.join(dirpath, filename)
            if os.path.exists(filepath):
                total += os.path.getsize(filepath)
    return total


def format_size(size_bytes: int) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–∞–∑–º–µ—Ä –≤ —á–∏—Ç–∞–µ–º—ã–π –≤–∏–¥"""
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if size_bytes < 1024.0:
            return f"{size_bytes:.2f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.2f} PB"


class PaperspaceUploader:
    """–ö–ª–∞—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ Paperspace –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, host: str = 'paperspace.com', path: str = '/storage/', user: Optional[str] = None):
        self.host = host
        self.path = path
        self.user = user
    
    def create_paperspace_training_archive(self, 
                               output_file: str,
                               include_ticks: bool = False,
                               include_cache: bool = False,
                               ask_ticks: bool = True) -> bool:
        """
        –°–æ–∑–¥–∞–µ—Ç tar.gz –∞—Ä—Ö–∏–≤ —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            output_file: –ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É
            include_ticks: –í–∫–ª—é—á–∞—Ç—å –ª–∏ —Ç–∏–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            include_cache: –í–∫–ª—é—á–∞—Ç—å –ª–∏ –∫—ç—à–∏
            ask_ticks: –°–ø—Ä–∞—à–∏–≤–∞—Ç—å –ª–∏ –æ –≤–∫–ª—é—á–µ–Ω–∏–∏ —Ç–∏–∫–æ–≤ (–µ—Å–ª–∏ include_ticks=True)
        """
        print("=" * 60)
        print("–£–ø–∞–∫–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ Paperspace")
        print("=" * 60)
        
        paths_to_include = []
        
        # CSV —Ñ–∞–π–ª—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)
        csv_files = [
            'workspace/prepared/features/gold_train.csv',
            'workspace/prepared/features/gold_val.csv',
            'workspace/prepared/features/gold_test.csv'
        ]
        
        for csv_file in csv_files:
            csv_path = Path(csv_file)
            if csv_path.exists():
                paths_to_include.append(csv_file)
                size = csv_path.stat().st_size
                print(f"‚úì –í–∫–ª—é—á–µ–Ω: {csv_file} ({format_size(size)})")
            else:
                print(f"‚ö† –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: {csv_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # –ö—ç—à–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if include_cache:
            cache_dir = Path('workspace/raw_data/cache')
            if cache_dir.exists():
                size = get_directory_size(cache_dir)
                paths_to_include.append(str(cache_dir))
                print(f"‚úì –í–∫–ª—é—á–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {cache_dir} ({format_size(size)})")
        
        # –¢–∏–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, —Å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ–º)
        if include_ticks:
            ticks_dir = Path('workspace/raw_data/ticks')
            if ticks_dir.exists():
                size = get_directory_size(ticks_dir)
                print(f"üìä –†–∞–∑–º–µ—Ä —Ç–∏–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {format_size(size)}")
                
                if ask_ticks:
                    response = input(f"–í–∫–ª—é—á–∏—Ç—å —Ç–∏–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ ({format_size(size)})? (y/n): ").strip().lower()
                    if response not in ['y', 'yes', '–¥–∞', '–¥', '']:
                        print("‚úó –¢–∏–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏—Å–∫–ª—é—á–µ–Ω—ã")
                        include_ticks = False
                
                if include_ticks:
                    paths_to_include.append(str(ticks_dir))
                    print(f"‚úì –í–∫–ª—é—á–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {ticks_dir}")
        
        if not paths_to_include:
            print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ø–∞–∫–æ–≤–∫–∏!")
            return False
        
        # –°–æ–∑–¥–∞–µ–º –∞—Ä—Ö–∏–≤
        print("\n" + "=" * 60)
        print("–°–æ–∑–¥–∞–Ω–∏–µ –∞—Ä—Ö–∏–≤–∞...")
        print("=" * 60)
        
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with tarfile.open(output_file, 'w:gz') as tar:
            for path_str in paths_to_include:
                path = Path(path_str)
                if path.exists():
                    print(f"–î–æ–±–∞–≤–ª–µ–Ω–∏–µ: {path_str}...")
                    tar.add(path_str, arcname=path_str, recursive=True)
                else:
                    print(f"‚ö† –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: {path_str} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
        
        archive_size = Path(output_file).stat().st_size
        print(f"\n‚úì –ê—Ä—Ö–∏–≤ —Å–æ–∑–¥–∞–Ω: {output_file}")
        print(f"  –†–∞–∑–º–µ—Ä: {format_size(archive_size)}")
        
        return True
    
    def upload_paperspace_training_data(self, archive_path: str, method: str = 'scp') -> bool:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∞—Ä—Ö–∏–≤ –Ω–∞ Paperspace
        
        Args:
            archive_path: –ü—É—Ç—å –∫ –∞—Ä—Ö–∏–≤—É
            method: –ú–µ—Ç–æ–¥ –∑–∞–≥—Ä—É–∑–∫–∏ ('scp' –∏–ª–∏ 'rsync')
        """
        if method == 'scp':
            return self._upload_via_scp(archive_path)
        elif method == 'rsync':
            return self._upload_via_rsync(archive_path)
        else:
            print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥: {method}")
            return False
    
    def _upload_via_scp(self, archive_path: str) -> bool:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∞—Ä—Ö–∏–≤ —á–µ—Ä–µ–∑ SCP"""
        print("\n" + "=" * 60)
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ Paperspace —á–µ—Ä–µ–∑ SCP")
        print("=" * 60)
        
        if self.user:
            scp_target = f"{self.user}@{self.host}:{self.path}"
        else:
            scp_target = f"{self.host}:{self.path}"
        
        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ {archive_path} -> {scp_target}")
        
        cmd = ['scp', archive_path, scp_target]
        print(f"–ö–æ–º–∞–Ω–¥–∞: {' '.join(cmd)}")
        
        try:
            subprocess.run(cmd, check=True)
            print("‚úì –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
            return True
        except subprocess.CalledProcessError as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
            return False
        except FileNotFoundError:
            print("‚ùå SCP –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ OpenSSH –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ rsync.")
            return False
    
    def _upload_via_rsync(self, archive_path: str) -> bool:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∞—Ä—Ö–∏–≤ —á–µ—Ä–µ–∑ RSYNC"""
        print("\n" + "=" * 60)
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ Paperspace —á–µ—Ä–µ–∑ RSYNC")
        print("=" * 60)
        
        if self.user:
            rsync_target = f"{self.user}@{self.host}:{self.path}"
        else:
            rsync_target = f"{self.host}:{self.path}"
        
        cmd = ['rsync', '-avz', '--progress', archive_path, rsync_target]
        print(f"–ö–æ–º–∞–Ω–¥–∞: {' '.join(cmd)}")
        
        try:
            subprocess.run(cmd, check=True)
            print("‚úì –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
            return True
        except subprocess.CalledProcessError as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
            return False
        except FileNotFoundError:
            print("‚ùå RSYNC –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ rsync.")
            return False
    
    # –ê–ª–∏–∞—Å—ã –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    create_training_archive = create_paperspace_training_archive
    upload_training_data = upload_paperspace_training_data


class PaperspaceDownloader:
    """–ö–ª–∞—Å—Å –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è —Å Paperspace"""
    
    def __init__(self, host: str = 'paperspace.com', path: str = '/storage/', user: Optional[str] = None):
        self.host = host
        self.path = path
        self.user = user
    
    def create_paperspace_results_archive(self, output_file: str) -> bool:
        """
        –°–æ–∑–¥–∞–µ—Ç tar.gz –∞—Ä—Ö–∏–≤ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è Paperspace
        
        Args:
            output_file: –ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É
        """
        print("=" * 60)
        print("–£–ø–∞–∫–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è")
        print("=" * 60)
        
        paths_to_include = []
        
        # –ú–æ–¥–µ–ª–∏
        models_dir = Path('workspace/models/checkpoints')
        if models_dir.exists():
            size = get_directory_size(models_dir)
            paths_to_include.append(str(models_dir))
            print(f"‚úì –í–∫–ª—é—á–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {models_dir} ({format_size(size)})")
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        metrics_dir = Path('workspace/models/metrics')
        if metrics_dir.exists():
            size = get_directory_size(metrics_dir)
            paths_to_include.append(str(metrics_dir))
            print(f"‚úì –í–∫–ª—é—á–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {metrics_dir} ({format_size(size)})")
        
        # Scalers
        scalers_dir = Path('workspace/prepared/scalers')
        if scalers_dir.exists():
            size = get_directory_size(scalers_dir)
            paths_to_include.append(str(scalers_dir))
            print(f"‚úì –í–∫–ª—é—á–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {scalers_dir} ({format_size(size)})")
        
        # TensorBoard –ª–æ–≥–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        logs_dir = Path('workspace/models/logs')
        if logs_dir.exists():
            size = get_directory_size(logs_dir)
            paths_to_include.append(str(logs_dir))
            print(f"‚úì –í–∫–ª—é—á–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {logs_dir} ({format_size(size)})")
        
        if not paths_to_include:
            print("‚ùå –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —É–ø–∞–∫–æ–≤–∫–∏!")
            return False
        
        # –°–æ–∑–¥–∞–µ–º –∞—Ä—Ö–∏–≤
        print("\n" + "=" * 60)
        print("–°–æ–∑–¥–∞–Ω–∏–µ –∞—Ä—Ö–∏–≤–∞...")
        print("=" * 60)
        
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with tarfile.open(output_file, 'w:gz') as tar:
            for path_str in paths_to_include:
                path = Path(path_str)
                if path.exists():
                    print(f"–î–æ–±–∞–≤–ª–µ–Ω–∏–µ: {path_str}...")
                    tar.add(path_str, arcname=path_str, recursive=True)
        
        archive_size = Path(output_file).stat().st_size
        print(f"\n‚úì –ê—Ä—Ö–∏–≤ —Å–æ–∑–¥–∞–Ω: {output_file}")
        print(f"  –†–∞–∑–º–µ—Ä: {format_size(archive_size)}")
        
        return True
    
    def download_paperspace_results(self, remote_archive: str, local_path: str = '.', method: str = 'scp') -> bool:
        """
        –°–∫–∞—á–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è —Å Paperspace
        
        Args:
            remote_archive: –ü—É—Ç—å –∫ –∞—Ä—Ö–∏–≤—É –Ω–∞ Paperspace
            local_path: –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            method: –ú–µ—Ç–æ–¥ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è ('scp' –∏–ª–∏ 'rsync')
        """
        if method == 'scp':
            return self._download_via_scp(remote_archive, local_path)
        elif method == 'rsync':
            return self._download_via_rsync(remote_archive, local_path)
        else:
            print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥: {method}")
            return False
    
    def _download_via_scp(self, remote_archive: str, local_path: str) -> bool:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –∞—Ä—Ö–∏–≤ —á–µ—Ä–µ–∑ SCP"""
        print("\n" + "=" * 60)
        print("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ SCP")
        print("=" * 60)
        
        if self.user:
            scp_source = f"{self.user}@{self.host}:{remote_archive}"
        else:
            scp_source = f"{self.host}:{remote_archive}"
        
        local_file = Path(local_path) / Path(remote_archive).name
        print(f"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ {scp_source} -> {local_file}")
        
        cmd = ['scp', scp_source, str(local_file)]
        print(f"–ö–æ–º–∞–Ω–¥–∞: {' '.join(cmd)}")
        
        try:
            subprocess.run(cmd, check=True)
            print("‚úì –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
            print(f"  –†–∞—Å–ø–∞–∫—É–π—Ç–µ –∞—Ä—Ö–∏–≤: tar -xzf {local_file.name}")
            return True
        except subprocess.CalledProcessError as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏: {e}")
            return False
        except FileNotFoundError:
            print("‚ùå SCP –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ OpenSSH.")
            return False
    
    def _download_via_rsync(self, remote_archive: str, local_path: str) -> bool:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –∞—Ä—Ö–∏–≤ —á–µ—Ä–µ–∑ RSYNC"""
        print("\n" + "=" * 60)
        print("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ RSYNC")
        print("=" * 60)
        
        if self.user:
            rsync_source = f"{self.user}@{self.host}:{remote_archive}"
        else:
            rsync_source = f"{self.host}:{remote_archive}"
        
        local_file = Path(local_path) / Path(remote_archive).name
        cmd = ['rsync', '-avz', '--progress', rsync_source, str(local_file)]
        print(f"–ö–æ–º–∞–Ω–¥–∞: {' '.join(cmd)}")
        
        try:
            subprocess.run(cmd, check=True)
            print("‚úì –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
            print(f"  –†–∞—Å–ø–∞–∫—É–π—Ç–µ –∞—Ä—Ö–∏–≤: tar -xzf {local_file.name}")
            return True
        except subprocess.CalledProcessError as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏: {e}")
            return False
        except FileNotFoundError:
            print("‚ùå RSYNC –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ rsync.")
            return False
    
    def list_paperspace_files(self, remote_path: str = None) -> bool:
        """
        –í—ã–≤–æ–¥–∏—Ç —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –Ω–∞ Paperspace
        
        Args:
            remote_path: –ü—É—Ç—å –Ω–∞ —É–¥–∞–ª–µ–Ω–Ω–æ–º —Å–µ—Ä–≤–µ—Ä–µ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é self.path)
        """
        if remote_path is None:
            remote_path = self.path
        
        print("\n" + "=" * 60)
        print("–°–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –Ω–∞ Paperspace")
        print("=" * 60)
        
        if self.user:
            ssh_target = f"{self.user}@{self.host}"
        else:
            ssh_target = self.host
        
        cmd = ['ssh', ssh_target, f'ls -lh {remote_path}']
        print(f"–ö–æ–º–∞–Ω–¥–∞: {' '.join(cmd)}")
        
        try:
            subprocess.run(cmd, check=True)
            return True
        except subprocess.CalledProcessError as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤: {e}")
            return False
        except FileNotFoundError:
            print("‚ùå SSH –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ OpenSSH.")
            return False
    
    # –ê–ª–∏–∞—Å—ã –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    create_results_archive = create_paperspace_results_archive
    download_results = download_paperspace_results
    list_remote_files = list_paperspace_files


class HuggingFaceUploader:
    """–ö–ª–∞—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ Hugging Face Hub"""
    
    def __init__(self, repo_id: str, token: Optional[str] = None):
        """
        Args:
            repo_id: ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –Ω–∞ Hugging Face (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'username/dataset-name')
            token: Hugging Face —Ç–æ–∫–µ–Ω (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è HF_TOKEN)
        """
        if not HF_AVAILABLE:
            raise ImportError("huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
        
        self.repo_id = repo_id
        self.api = HfApi(token=token)
        self.token = token or os.getenv('HF_TOKEN')
        
        if not self.token:
            print("‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: HF_TOKEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ú–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è.")
    
    def upload_hf_ticks(self, ticks_dir: str = 'workspace/raw_data/ticks', 
                     commit_message: str = "Upload tick data") -> bool:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–∏–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ Hugging Face
        
        Args:
            ticks_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ç–∏–∫–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            commit_message: –°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞
        """
        print("=" * 60)
        print("–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–∏–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ Hugging Face")
        print("=" * 60)
        
        ticks_path = Path(ticks_dir)
        if not ticks_path.exists():
            print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {ticks_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
            return False
        
        size = get_directory_size(ticks_path)
        print(f"üìä –†–∞–∑–º–µ—Ä —Ç–∏–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {format_size(size)}")
        print(f"üìÅ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {self.repo_id}")
        print(f"üìÇ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {ticks_dir}")
        
        try:
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            temp_dir = Path('temp_hf_upload')
            temp_ticks_dir = temp_dir / 'ticks'
            temp_ticks_dir.mkdir(parents=True, exist_ok=True)
            
            # –ö–æ–ø–∏—Ä—É–µ–º —Ç–∏–∫–∏ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            print(f"\n–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
            shutil.copytree(ticks_path, temp_ticks_dir, dirs_exist_ok=True)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞ Hugging Face
            print(f"\n–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ Hugging Face...")
            upload_folder(
                folder_path=str(temp_dir),
                repo_id=self.repo_id,
                repo_type="dataset",
                token=self.token,
                commit_message=commit_message
            )
            
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            shutil.rmtree(temp_dir, ignore_errors=True)
            
            print(f"\n‚úì –¢–∏–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –Ω–∞ Hugging Face!")
            print(f"  –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: https://huggingface.co/datasets/{self.repo_id}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
            shutil.rmtree(temp_dir, ignore_errors=True)
            return False
    
    def upload_hf_feature_analysis(self, 
                               analysis_dir: str = 'workspace/analysis-of-features',
                               commit_message: str = "Upload feature analysis results") -> bool:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π (--analyze-features) –Ω–∞ Hugging Face
        
        Args:
            analysis_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: workspace/analysis-of-features)
            commit_message: –°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞
        """
        print("=" * 60)
        print("–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π –Ω–∞ Hugging Face")
        print("=" * 60)
        
        analysis_path = Path(analysis_dir)
        if not analysis_path.exists():
            print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {analysis_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
            print("   –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python full_pipeline.py --analyze-features")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        expected_files = [
            'feature_statistics.csv',
            'feature_importance.csv',
            'outliers_analysis.csv',
            'feature_by_class_statistics.csv',
            'feature_analysis_report.html'
        ]
        
        found_files = []
        for file_name in expected_files:
            file_path = analysis_path / file_name
            if file_path.exists():
                found_files.append(file_name)
                size = file_path.stat().st_size
                print(f"‚úì –ù–∞–π–¥–µ–Ω: {file_name} ({format_size(size)})")
            else:
                print(f"‚ö† –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç: {file_name}")
        
        if not found_files:
            print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞!")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ plots (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        plots_dir = analysis_path / 'plots'
        has_plots = plots_dir.exists() and plots_dir.is_dir()
        if has_plots:
            plots_size = get_directory_size(plots_dir)
            print(f"‚úì –ù–∞–π–¥–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è plots ({format_size(plots_size)})")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ excluded_features.txt –≤ workspace
        excluded_features_file = Path('workspace/excluded_features.txt')
        has_excluded = excluded_features_file.exists()
        if has_excluded:
            size = excluded_features_file.stat().st_size
            print(f"‚úì –ù–∞–π–¥–µ–Ω: excluded_features.txt ({format_size(size)})")
        
        size = get_directory_size(analysis_path)
        print(f"\nüìä –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {format_size(size)}")
        print(f"üìÅ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {self.repo_id}")
        print(f"üìÇ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {analysis_dir}")
        
        try:
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            temp_dir = Path('temp_hf_upload')
            temp_analysis_dir = temp_dir / 'analysis-of-features'
            temp_analysis_dir.mkdir(parents=True, exist_ok=True)
            
            # –ö–æ–ø–∏—Ä—É–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏–∑ analysis_dir
            print(f"\n–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
            for item in analysis_path.iterdir():
                if item.is_file():
                    shutil.copy2(item, temp_analysis_dir / item.name)
                    print(f"  –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω —Ñ–∞–π–ª: {item.name}")
                elif item.is_dir():
                    shutil.copytree(item, temp_analysis_dir / item.name, dirs_exist_ok=True)
                    print(f"  –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {item.name}")
            
            # –ö–æ–ø–∏—Ä—É–µ–º excluded_features.txt –∏–∑ workspace, –µ—Å–ª–∏ –æ–Ω —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            if has_excluded:
                temp_workspace_dir = temp_dir / 'workspace'
                temp_workspace_dir.mkdir(parents=True, exist_ok=True)
                shutil.copy2(excluded_features_file, temp_workspace_dir / 'excluded_features.txt')
                print(f"  –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω —Ñ–∞–π–ª: workspace/excluded_features.txt")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞ Hugging Face
            print(f"\n–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ Hugging Face...")
            upload_folder(
                folder_path=str(temp_dir),
                repo_id=self.repo_id,
                repo_type="dataset",
                token=self.token,
                commit_message=commit_message
            )
            
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            shutil.rmtree(temp_dir, ignore_errors=True)
            
            print(f"\n‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –Ω–∞ Hugging Face!")
            print(f"  –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: https://huggingface.co/datasets/{self.repo_id}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
            import traceback
            traceback.print_exc()
            shutil.rmtree(temp_dir, ignore_errors=True)
            return False
    
    def upload_hf_training_data(self,
                             include_scalers: bool = True,
                             include_cache: bool = False,
                             commit_message: str = "Upload training data") -> bool:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ Hugging Face (–±–µ–∑ —Ç–∏–∫–æ–≤)
        
        Args:
            include_scalers: –í–∫–ª—é—á–∞—Ç—å –ª–∏ scalers
            include_cache: –í–∫–ª—é—á–∞—Ç—å –ª–∏ –∫—ç—à–∏
            commit_message: –°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞
        """
        print("=" * 60)
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ Hugging Face")
        print("=" * 60)
        
        paths_to_include = []
        
        # CSV —Ñ–∞–π–ª—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)
        csv_files = [
            'workspace/prepared/features/gold_train.csv',
            'workspace/prepared/features/gold_val.csv',
            'workspace/prepared/features/gold_test.csv'
        ]
        
        for csv_file in csv_files:
            csv_path = Path(csv_file)
            if csv_path.exists():
                paths_to_include.append(csv_file)
                size = csv_path.stat().st_size
                print(f"‚úì –í–∫–ª—é—á–µ–Ω: {csv_file} ({format_size(size)})")
            else:
                print(f"‚ö† –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: {csv_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # Scalers (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if include_scalers:
            scalers_dir = Path('workspace/prepared/scalers')
            if scalers_dir.exists():
                size = get_directory_size(scalers_dir)
                paths_to_include.append(str(scalers_dir))
                print(f"‚úì –í–∫–ª—é—á–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {scalers_dir} ({format_size(size)})")
        
        # –ö—ç—à–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if include_cache:
            cache_dir = Path('workspace/raw_data/cache')
            if cache_dir.exists():
                size = get_directory_size(cache_dir)
                paths_to_include.append(str(cache_dir))
                print(f"‚úì –í–∫–ª—é—á–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {cache_dir} ({format_size(size)})")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ excluded_features.txt –≤ workspace
        excluded_features_file = Path('workspace/excluded_features.txt')
        has_excluded = excluded_features_file.exists()
        if has_excluded:
            size = excluded_features_file.stat().st_size
            print(f"‚úì –ù–∞–π–¥–µ–Ω: excluded_features.txt ({format_size(size)})")
        
        if not paths_to_include:
            print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏!")
            return False
        
        print(f"\nüìÅ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {self.repo_id}")
        
        try:
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            temp_dir = Path('temp_hf_upload')
            temp_dir.mkdir(parents=True, exist_ok=True)
            
            # –ö–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            print(f"\n–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
            for path_str in paths_to_include:
                path = Path(path_str)
                if path.exists():
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
                    rel_path = path.relative_to(Path('workspace').parent)
                    dest_path = temp_dir / rel_path
                    dest_path.parent.mkdir(parents=True, exist_ok=True)
                    
                    if path.is_file():
                        shutil.copy2(path, dest_path)
                    else:
                        shutil.copytree(path, dest_path, dirs_exist_ok=True)
                    print(f"  –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ: {path_str}")
            
            # –ö–æ–ø–∏—Ä—É–µ–º excluded_features.txt, –µ—Å–ª–∏ –æ–Ω —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            if has_excluded:
                workspace_dest = temp_dir / 'workspace'
                workspace_dest.mkdir(parents=True, exist_ok=True)
                shutil.copy2(excluded_features_file, workspace_dest / 'excluded_features.txt')
                print(f"  –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ: workspace/excluded_features.txt")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞ Hugging Face
            print(f"\n–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ Hugging Face...")
            upload_folder(
                folder_path=str(temp_dir),
                repo_id=self.repo_id,
                repo_type="dataset",
                token=self.token,
                commit_message=commit_message
            )
            
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            shutil.rmtree(temp_dir, ignore_errors=True)
            
            print(f"\n‚úì –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –Ω–∞ Hugging Face!")
            print(f"  –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: https://huggingface.co/datasets/{self.repo_id}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
            import traceback
            traceback.print_exc()
            shutil.rmtree(temp_dir, ignore_errors=True)
            return False
    
    # –ê–ª–∏–∞—Å—ã –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    upload_ticks = upload_hf_ticks
    upload_feature_analysis = upload_hf_feature_analysis
    upload_training_data = upload_hf_training_data


class HuggingFaceDeleter:
    """–ö–ª–∞—Å—Å –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ Hugging Face Hub"""
    
    def __init__(self, repo_id: str, token: Optional[str] = None):
        """
        Args:
            repo_id: ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –Ω–∞ Hugging Face (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'username/dataset-name')
            token: Hugging Face —Ç–æ–∫–µ–Ω (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è HF_TOKEN)
        """
        if not HF_AVAILABLE:
            raise ImportError("huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
        
        self.repo_id = repo_id
        self.api = HfApi(token=token)
        self.token = token or os.getenv('HF_TOKEN')
        
        if not self.token:
            print("‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: HF_TOKEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ú–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è.")
    
    def _list_repo_files(self) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏"""
        try:
            files = self.api.list_repo_files(
                repo_id=self.repo_id,
                repo_type="dataset",
                token=self.token
            )
            return files
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤: {e}")
            return []
    
    def delete_hf_ticks(self, commit_message: str = "Delete tick data") -> bool:
        """
        –£–¥–∞–ª—è–µ—Ç —Ç–∏–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
        
        Args:
            commit_message: –°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞
        """
        print("=" * 60)
        print("–£–¥–∞–ª–µ–Ω–∏–µ —Ç–∏–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ Hugging Face")
        print("=" * 60)
        print(f"üìÅ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {self.repo_id}")
        
        try:
            files = self._list_repo_files()
            tick_files = [f for f in files if f.startswith('ticks/')]
            
            if not tick_files:
                print("‚úì –¢–∏–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏")
                return True
            
            print(f"\n–ù–∞–π–¥–µ–Ω–æ {len(tick_files)} —Ñ–∞–π–ª–æ–≤/–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è:")
            for file in tick_files[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
                print(f"  - {file}")
            if len(tick_files) > 10:
                print(f"  ... –∏ –µ—â–µ {len(tick_files) - 10} —Ñ–∞–π–ª–æ–≤")
            
            # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª—ã
            print(f"\n–£–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤...")
            for file in tick_files:
                try:
                    self.api.delete_file(
                        path_in_repo=file,
                        repo_id=self.repo_id,
                        repo_type="dataset",
                        token=self.token,
                        commit_message=commit_message if file == tick_files[0] else None
                    )
                except Exception as e:
                    print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ {file}: {e}")
            
            print(f"\n‚úì –¢–∏–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω—ã –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è!")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def delete_hf_training_data(self, 
                               include_scalers: bool = True,
                               include_cache: bool = True,
                               commit_message: str = "Delete training data") -> bool:
        """
        –£–¥–∞–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
        
        Args:
            include_scalers: –£–¥–∞–ª—è—Ç—å –ª–∏ scalers
            include_cache: –£–¥–∞–ª—è—Ç—å –ª–∏ –∫—ç—à–∏
            commit_message: –°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞
        """
        print("=" * 60)
        print("–£–¥–∞–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏–∑ Hugging Face")
        print("=" * 60)
        print(f"üìÅ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {self.repo_id}")
        
        try:
            files = self._list_repo_files()
            files_to_delete = []
            
            # CSV —Ñ–∞–π–ª—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            training_patterns = [
                'workspace/prepared/features/gold_train.csv',
                'workspace/prepared/features/gold_val.csv',
                'workspace/prepared/features/gold_test.csv'
            ]
            
            # Scalers
            if include_scalers:
                scaler_files = [f for f in files if f.startswith('workspace/prepared/scalers/')]
                files_to_delete.extend(scaler_files)
            
            # –ö—ç—à–∏
            if include_cache:
                cache_files = [f for f in files if f.startswith('workspace/raw_data/cache/')]
                files_to_delete.extend(cache_files)
            
            # CSV —Ñ–∞–π–ª—ã
            for pattern in training_patterns:
                if pattern in files:
                    files_to_delete.append(pattern)
            
            # excluded_features.txt
            excluded_file = 'workspace/excluded_features.txt'
            if excluded_file in files:
                files_to_delete.append(excluded_file)
            
            if not files_to_delete:
                print("‚úì –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏")
                return True
            
            print(f"\n–ù–∞–π–¥–µ–Ω–æ {len(files_to_delete)} —Ñ–∞–π–ª–æ–≤/–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è:")
            for file in files_to_delete[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
                print(f"  - {file}")
            if len(files_to_delete) > 10:
                print(f"  ... –∏ –µ—â–µ {len(files_to_delete) - 10} —Ñ–∞–π–ª–æ–≤")
            
            # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª—ã
            print(f"\n–£–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤...")
            for i, file in enumerate(files_to_delete):
                try:
                    self.api.delete_file(
                        path_in_repo=file,
                        repo_id=self.repo_id,
                        repo_type="dataset",
                        token=self.token,
                        commit_message=commit_message if i == 0 else None
                    )
                except Exception as e:
                    print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ {file}: {e}")
            
            print(f"\n‚úì –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω—ã –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è!")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def delete_hf_feature_analysis(self, commit_message: str = "Delete feature analysis results") -> bool:
        """
        –£–¥–∞–ª—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
        
        Args:
            commit_message: –°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞
        """
        print("=" * 60)
        print("–£–¥–∞–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π –∏–∑ Hugging Face")
        print("=" * 60)
        print(f"üìÅ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {self.repo_id}")
        
        try:
            files = self._list_repo_files()
            analysis_files = [f for f in files if f.startswith('analysis-of-features/')]
            
            if not analysis_files:
                print("‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏")
                return True
            
            print(f"\n–ù–∞–π–¥–µ–Ω–æ {len(analysis_files)} —Ñ–∞–π–ª–æ–≤/–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è:")
            for file in analysis_files[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
                print(f"  - {file}")
            if len(analysis_files) > 10:
                print(f"  ... –∏ –µ—â–µ {len(analysis_files) - 10} —Ñ–∞–π–ª–æ–≤")
            
            # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª—ã
            print(f"\n–£–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤...")
            for i, file in enumerate(analysis_files):
                try:
                    self.api.delete_file(
                        path_in_repo=file,
                        repo_id=self.repo_id,
                        repo_type="dataset",
                        token=self.token,
                        commit_message=commit_message if i == 0 else None
                    )
                except Exception as e:
                    print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ {file}: {e}")
            
            print(f"\n‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω—ã –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è!")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def delete_all_data(self, commit_message: str = "Delete all dataset data") -> bool:
        """
        –£–¥–∞–ª—è–µ—Ç –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–æ—á–∏—â–∞–µ—Ç —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –¥–ª—è –Ω–æ–≤—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫)
        
        Args:
            commit_message: –°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞
        """
        print("=" * 60)
        print("–£–¥–∞–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ Hugging Face")
        print("=" * 60)
        print(f"üìÅ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {self.repo_id}")
        print("‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –≠—Ç–æ —É–¥–∞–ª–∏—Ç –í–°–ï –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è!")
        
        try:
            files = self._list_repo_files()
            
            if not files:
                print("‚úì –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π —É–∂–µ –ø—É—Å—Ç")
                return True
            
            print(f"\n–ù–∞–π–¥–µ–Ω–æ {len(files)} —Ñ–∞–π–ª–æ–≤/–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è:")
            for file in files[:20]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 20
                print(f"  - {file}")
            if len(files) > 20:
                print(f"  ... –∏ –µ—â–µ {len(files) - 20} —Ñ–∞–π–ª–æ–≤")
            
            # –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
            response = input("\n–í—ã —É–≤–µ—Ä–µ–Ω—ã, —á—Ç–æ —Ö–æ—Ç–∏—Ç–µ —É–¥–∞–ª–∏—Ç—å –í–°–ï –¥–∞–Ω–Ω—ã–µ? (yes/no): ").strip().lower()
            if response != 'yes':
                print("‚ùå –£–¥–∞–ª–µ–Ω–∏–µ –æ—Ç–º–µ–Ω–µ–Ω–æ")
                return False
            
            # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª—ã
            print(f"\n–£–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤...")
            for i, file in enumerate(files):
                try:
                    self.api.delete_file(
                        path_in_repo=file,
                        repo_id=self.repo_id,
                        repo_type="dataset",
                        token=self.token,
                        commit_message=commit_message if i == 0 else None
                    )
                    if (i + 1) % 10 == 0:
                        print(f"  –£–¥–∞–ª–µ–Ω–æ {i + 1}/{len(files)} —Ñ–∞–π–ª–æ–≤...")
                except Exception as e:
                    print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ {file}: {e}")
            
            print(f"\n‚úì –í—Å–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω—ã –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è!")
            print(f"  –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –≥–æ—Ç–æ–≤ –¥–ª—è –Ω–æ–≤—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏: {e}")
            import traceback
            traceback.print_exc()
            return False


class HuggingFaceDownloader:
    """–ö–ª–∞—Å—Å –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —Å Hugging Face Hub"""
    
    def __init__(self, repo_id: str, token: Optional[str] = None):
        """
        Args:
            repo_id: ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –Ω–∞ Hugging Face (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'username/dataset-name')
            token: Hugging Face —Ç–æ–∫–µ–Ω (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è HF_TOKEN)
        """
        if not HF_AVAILABLE:
            raise ImportError("huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
        
        self.repo_id = repo_id
        self.api = HfApi(token=token)
        self.token = token or os.getenv('HF_TOKEN')
    
    def download_hf_ticks(self, local_dir: str = 'workspace/raw_data/ticks') -> bool:
        """
        –°–∫–∞—á–∏–≤–∞–µ—Ç —Ç–∏–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å Hugging Face
        
        Args:
            local_dir: –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        print("=" * 60)
        print("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ç–∏–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å Hugging Face")
        print("=" * 60)
        
        print(f"üìÅ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {self.repo_id}")
        print(f"üìÇ –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {local_dir}")
        
        try:
            local_path = Path(local_dir)
            local_path.mkdir(parents=True, exist_ok=True)
            
            # –°–∫–∞—á–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–∞–ø–∫—É ticks
            print(f"\n–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (—Ç–æ–ª—å–∫–æ –ø–∞–ø–∫–∞ ticks/)...")
            temp_dir = Path('temp_hf_download')
            temp_dir.mkdir(exist_ok=True)
            
            try:
                downloaded_path = snapshot_download(
                    repo_id=self.repo_id,
                    repo_type="dataset",
                    local_dir=str(temp_dir),
                    token=self.token,
                    allow_patterns=["ticks/**"]  # –°–∫–∞—á–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–∞–ø–∫—É ticks
                )
                
                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                downloaded_path = Path(downloaded_path)
                ticks_source = downloaded_path / 'ticks'
                
                if ticks_source.exists() and ticks_source.is_dir():
                    print(f"  –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏...")
                    for item in ticks_source.iterdir():
                        dest = local_path / item.name
                        if item.is_file():
                            if dest.exists():
                                dest.unlink()
                            shutil.move(str(item), str(dest))
                        else:
                            if dest.exists():
                                shutil.rmtree(dest)
                            shutil.move(str(item), str(dest))
                else:
                    print("‚ö†Ô∏è  –ü–∞–ø–∫–∞ ticks –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏")
                    print(f"   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è. –û–∂–∏–¥–∞–µ—Ç—Å—è: ticks/")
                    return False
            finally:
                # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
                if temp_dir.exists():
                    shutil.rmtree(temp_dir, ignore_errors=True)
            
            print(f"\n‚úì –¢–∏–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω—ã!")
            print(f"  –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {local_dir}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def download_hf_training_data(self, local_dir: str = 'workspace') -> bool:
        """
        –°–∫–∞—á–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å Hugging Face
        
        Args:
            local_dir: –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        print("=" * 60)
        print("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å Hugging Face")
        print("=" * 60)
        
        print(f"üìÅ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {self.repo_id}")
        print(f"üìÇ –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {local_dir}")
        
        try:
            local_path = Path(local_dir)
            local_path.mkdir(parents=True, exist_ok=True)
            
            # –°–∫–∞—á–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (CSV —Ñ–∞–π–ª—ã, scalers, cache, excluded_features.txt)
            print(f"\n–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (—Ç–æ–ª—å–∫–æ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è)...")
            temp_dir = Path('temp_hf_download')
            temp_dir.mkdir(exist_ok=True)
            
            try:
                downloaded_path = snapshot_download(
                    repo_id=self.repo_id,
                    repo_type="dataset",
                    local_dir=str(temp_dir),
                    token=self.token,
                    allow_patterns=[
                        "workspace/prepared/features/*.csv",
                        "workspace/prepared/scalers/**",
                        "workspace/raw_data/cache/**",
                        "workspace/excluded_features.txt"  # –°–∫–∞—á–∏–≤–∞–µ–º excluded_features.txt
                    ]  # –°–∫–∞—á–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                )
                
                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, —Å–æ—Ö—Ä–∞–Ω—è—è —Å—Ç—Ä—É–∫—Ç—É—Ä—É
                downloaded_path = Path(downloaded_path)
                workspace_source = downloaded_path / 'workspace'
                
                if workspace_source.exists():
                    print(f"  –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏...")
                    found_any = False
                    
                    # –ü–µ—Ä–µ–º–µ—â–∞–µ–º prepared/features/*.csv
                    features_source = workspace_source / 'prepared' / 'features'
                    if features_source.exists():
                        features_dest = local_path / 'prepared' / 'features'
                        features_dest.mkdir(parents=True, exist_ok=True)
                        csv_files = list(features_source.glob('*.csv'))
                        if csv_files:
                            found_any = True
                            for csv_file in csv_files:
                                dest_file = features_dest / csv_file.name
                                if dest_file.exists():
                                    dest_file.unlink()
                                shutil.move(str(csv_file), str(dest_file))
                    
                    # –ü–µ—Ä–µ–º–µ—â–∞–µ–º prepared/scalers
                    scalers_source = workspace_source / 'prepared' / 'scalers'
                    if scalers_source.exists():
                        found_any = True
                        scalers_dest = local_path / 'prepared' / 'scalers'
                        if scalers_dest.exists():
                            shutil.rmtree(scalers_dest)
                        scalers_dest.parent.mkdir(parents=True, exist_ok=True)
                        shutil.move(str(scalers_source), str(scalers_dest))
                    
                    # –ü–µ—Ä–µ–º–µ—â–∞–µ–º raw_data/cache
                    cache_source = workspace_source / 'raw_data' / 'cache'
                    if cache_source.exists():
                        found_any = True
                        cache_dest = local_path / 'raw_data' / 'cache'
                        if cache_dest.exists():
                            shutil.rmtree(cache_dest)
                        cache_dest.parent.mkdir(parents=True, exist_ok=True)
                        shutil.move(str(cache_source), str(cache_dest))
                    
                    # –ü–µ—Ä–µ–º–µ—â–∞–µ–º excluded_features.txt, –µ—Å–ª–∏ –æ–Ω –Ω–∞–π–¥–µ–Ω
                    excluded_source = workspace_source / 'excluded_features.txt'
                    if excluded_source.exists():
                        excluded_dest = local_path / 'excluded_features.txt'
                        if excluded_dest.exists():
                            excluded_dest.unlink()
                        excluded_dest.parent.mkdir(parents=True, exist_ok=True)
                        shutil.move(str(excluded_source), str(excluded_dest))
                        print(f"  ‚úì –ü–µ—Ä–µ–º–µ—â–µ–Ω excluded_features.txt –≤ workspace/")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ö–æ—Ç—è –±—ã —á—Ç–æ-—Ç–æ –±—ã–ª–æ —Å–∫–∞—á–∞–Ω–æ
                    if not found_any:
                        print("‚ö†Ô∏è  –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏")
                        print(f"   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è.")
                        print(f"   –û–∂–∏–¥–∞–µ—Ç—Å—è: workspace/prepared/features/*.csv –∏–ª–∏ workspace/prepared/scalers/")
                        return False
                else:
                    print("‚ö†Ô∏è  –°—Ç—Ä—É–∫—Ç—É—Ä–∞ workspace –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏")
                    print(f"   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è. –û–∂–∏–¥–∞–µ—Ç—Å—è: workspace/")
                    return False
            finally:
                # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
                if temp_dir.exists():
                    shutil.rmtree(temp_dir, ignore_errors=True)
            
            print(f"\n‚úì –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω—ã!")
            print(f"  –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {local_dir}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def download_hf_feature_analysis(self, local_dir: str = 'workspace/analysis-of-features') -> bool:
        """
        –°–∫–∞—á–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π —Å Hugging Face
        
        Args:
            local_dir: –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: workspace/analysis-of-features)
        """
        print("=" * 60)
        print("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π —Å Hugging Face")
        print("=" * 60)
        
        print(f"üìÅ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {self.repo_id}")
        print(f"üìÇ –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {local_dir}")
        
        try:
            local_path = Path(local_dir)
            local_path.mkdir(parents=True, exist_ok=True)
            
            # –°–∫–∞—á–∏–≤–∞–µ–º –ø–∞–ø–∫—É analysis-of-features –∏ excluded_features.txt
            print(f"\n–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (–ø–∞–ø–∫–∞ analysis-of-features/ –∏ excluded_features.txt)...")
            temp_dir = Path('temp_hf_download')
            temp_dir.mkdir(exist_ok=True)
            
            try:
                downloaded_path = snapshot_download(
                    repo_id=self.repo_id,
                    repo_type="dataset",
                    local_dir=str(temp_dir),
                    token=self.token,
                    allow_patterns=[
                        "analysis-of-features/**",  # –°–∫–∞—á–∏–≤–∞–µ–º –ø–∞–ø–∫—É analysis-of-features
                        "workspace/excluded_features.txt"  # –°–∫–∞—á–∏–≤–∞–µ–º excluded_features.txt
                    ]
                )
                
                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                downloaded_path = Path(downloaded_path)
                analysis_source = downloaded_path / 'analysis-of-features'
                
                found_analysis = False
                if analysis_source.exists() and analysis_source.is_dir():
                    found_analysis = True
                    print(f"  –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏...")
                    for item in analysis_source.iterdir():
                        dest = local_path / item.name
                        if item.is_file():
                            if dest.exists():
                                dest.unlink()
                            shutil.move(str(item), str(dest))
                        else:
                            if dest.exists():
                                shutil.rmtree(dest)
                            shutil.move(str(item), str(dest))
                
                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º excluded_features.txt –≤ workspace, –µ—Å–ª–∏ –æ–Ω –Ω–∞–π–¥–µ–Ω
                excluded_source = downloaded_path / 'workspace' / 'excluded_features.txt'
                if excluded_source.exists():
                    workspace_path = Path('workspace')
                    workspace_path.mkdir(parents=True, exist_ok=True)
                    excluded_dest = workspace_path / 'excluded_features.txt'
                    if excluded_dest.exists():
                        excluded_dest.unlink()
                    shutil.move(str(excluded_source), str(excluded_dest))
                    print(f"  ‚úì –ü–µ—Ä–µ–º–µ—â–µ–Ω excluded_features.txt –≤ workspace/")
                
                if not found_analysis:
                    # –ü—Ä–æ–±—É–µ–º —Å—Ç–∞—Ä—ã–π –ø—É—Ç—å –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                    old_analysis_source = downloaded_path / 'features-analysis'
                    if old_analysis_source.exists() and old_analysis_source.is_dir():
                        found_analysis = True
                        print(f"  –ù–∞–π–¥–µ–Ω–∞ –ø–∞–ø–∫–∞ features-analysis (—Å—Ç–∞—Ä–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ), –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ...")
                        for item in old_analysis_source.iterdir():
                            dest = local_path / item.name
                            if item.is_file():
                                if dest.exists():
                                    dest.unlink()
                                shutil.move(str(item), str(dest))
                            else:
                                if dest.exists():
                                    shutil.rmtree(dest)
                                shutil.move(str(item), str(dest))
                
                if not found_analysis:
                    print("‚ö†Ô∏è  –ü–∞–ø–∫–∞ analysis-of-features –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏")
                    print(f"   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è. –û–∂–∏–¥–∞–µ—Ç—Å—è: analysis-of-features/")
                    return False
            finally:
                # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
                if temp_dir.exists():
                    shutil.rmtree(temp_dir, ignore_errors=True)
            
            print(f"\n‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω—ã!")
            print(f"  –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {local_dir}")
            print(f"\n  –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã:")
            for item in local_path.iterdir():
                if item.is_file():
                    size = item.stat().st_size
                    print(f"    - {item.name} ({format_size(size)})")
                elif item.is_dir():
                    size = get_directory_size(item)
                    print(f"    - {item.name}/ ({format_size(size)})")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    # –ê–ª–∏–∞—Å—ã –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    download_ticks = download_hf_ticks
    download_training_data = download_hf_training_data
    download_feature_analysis = download_hf_feature_analysis


def main():
    parser = argparse.ArgumentParser(
        description='–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –æ–±–ª–∞—á–Ω—ã–º–∏ —Å–µ—Ä–≤–∏—Å–∞–º–∏ (Paperspace –∏ Hugging Face)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

Paperspace:
  python cloud_services.py upload-training --host paperspace.com --path /storage/
  python cloud_services.py create-training-archive --output training_data.tar.gz
  python cloud_services.py download-results --host paperspace.com --path /storage/results.tar.gz
  python cloud_services.py create-results-archive --output results.tar.gz
  python cloud_services.py list-remote-files --host paperspace.com --path /storage/

Hugging Face:
  python cloud_services.py hf-upload-ticks --repo-id username/dataset-name
  python cloud_services.py hf-download-ticks --repo-id username/dataset-name
  python cloud_services.py hf-upload-training --repo-id username/dataset-name
  python cloud_services.py hf-download-training --repo-id username/dataset-name
  python cloud_services.py hf-upload-features --repo-id username/dataset-name
  python cloud_services.py hf-download-features --repo-id username/dataset-name
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='–ö–æ–º–∞–Ω–¥–∞')
    
    # Upload training data
    upload_parser = subparsers.add_parser('upload-training', help='–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è')
    upload_parser.add_argument('--host', type=str, default='paperspace.com', help='–•–æ—Å—Ç Paperspace')
    upload_parser.add_argument('--path', type=str, default='/storage/', help='–ü—É—Ç—å –Ω–∞ Paperspace')
    upload_parser.add_argument('--user', type=str, default=None, help='–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å')
    upload_parser.add_argument('--method', type=str, choices=['scp', 'rsync'], default='scp', help='–ú–µ—Ç–æ–¥ –∑–∞–≥—Ä—É–∑–∫–∏')
    upload_parser.add_argument('--include-ticks', action='store_true', help='–í–∫–ª—é—á–∏—Ç—å —Ç–∏–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ')
    upload_parser.add_argument('--include-cache', action='store_true', help='–í–∫–ª—é—á–∏—Ç—å –∫—ç—à–∏')
    upload_parser.add_argument('--no-ask-ticks', action='store_true', help='–ù–µ —Å–ø—Ä–∞—à–∏–≤–∞—Ç—å –æ —Ç–∏–∫–∞—Ö')
    
    # Create training archive
    create_training_parser = subparsers.add_parser('create-training-archive', help='–°–æ–∑–¥–∞—Ç—å –∞—Ä—Ö–∏–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è')
    create_training_parser.add_argument('--output', '-o', type=str,
                                      default=f'training_data_{datetime.now().strftime("%Y%m%d_%H%M%S")}.tar.gz',
                                      help='–ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É –∞—Ä—Ö–∏–≤—É')
    create_training_parser.add_argument('--include-ticks', action='store_true', help='–í–∫–ª—é—á–∏—Ç—å —Ç–∏–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ')
    create_training_parser.add_argument('--include-cache', action='store_true', help='–í–∫–ª—é—á–∏—Ç—å –∫—ç—à–∏')
    create_training_parser.add_argument('--no-ask-ticks', action='store_true', help='–ù–µ —Å–ø—Ä–∞—à–∏–≤–∞—Ç—å –æ —Ç–∏–∫–∞—Ö')
    
    # Download results
    download_parser = subparsers.add_parser('download-results', help='–°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è')
    download_parser.add_argument('--host', type=str, default='paperspace.com', help='–•–æ—Å—Ç Paperspace')
    download_parser.add_argument('--path', type=str, required=True, help='–ü—É—Ç—å –∫ –∞—Ä—Ö–∏–≤—É –Ω–∞ Paperspace')
    download_parser.add_argument('--user', type=str, default=None, help='–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å')
    download_parser.add_argument('--method', type=str, choices=['scp', 'rsync'], default='scp', help='–ú–µ—Ç–æ–¥ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è')
    download_parser.add_argument('--local-path', type=str, default='.', help='–õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è')
    
    # Create results archive
    create_results_parser = subparsers.add_parser('create-results-archive', help='–°–æ–∑–¥–∞—Ç—å –∞—Ä—Ö–∏–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
    create_results_parser.add_argument('--output', '-o', type=str,
                                     default=f'results_{datetime.now().strftime("%Y%m%d_%H%M%S")}.tar.gz',
                                     help='–ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É –∞—Ä—Ö–∏–≤—É')
    
    # List remote files
    list_parser = subparsers.add_parser('list-remote-files', help='–°–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –Ω–∞ Paperspace')
    list_parser.add_argument('--host', type=str, default='paperspace.com', help='–•–æ—Å—Ç Paperspace')
    list_parser.add_argument('--path', type=str, default='/storage/', help='–ü—É—Ç—å –Ω–∞ Paperspace')
    list_parser.add_argument('--user', type=str, default=None, help='–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å')
    
    # Hugging Face: Upload ticks
    hf_upload_ticks_parser = subparsers.add_parser('hf-upload-ticks', help='–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ç–∏–∫–∏ –Ω–∞ Hugging Face')
    hf_upload_ticks_parser.add_argument('--repo-id', type=str, required=True, help='ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (username/dataset-name)')
    hf_upload_ticks_parser.add_argument('--token', type=str, default=None, help='Hugging Face —Ç–æ–∫–µ–Ω (–∏–ª–∏ HF_TOKEN env var)')
    hf_upload_ticks_parser.add_argument('--ticks-dir', type=str, default='workspace/raw_data/ticks', help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ç–∏–∫–∞–º–∏')
    hf_upload_ticks_parser.add_argument('--commit-message', type=str, default='Upload tick data', help='–°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞')
    
    # Hugging Face: Download ticks
    hf_download_ticks_parser = subparsers.add_parser('hf-download-ticks', help='–°–∫–∞—á–∞—Ç—å —Ç–∏–∫–∏ —Å Hugging Face')
    hf_download_ticks_parser.add_argument('--repo-id', type=str, required=True, help='ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (username/dataset-name)')
    hf_download_ticks_parser.add_argument('--token', type=str, default=None, help='Hugging Face —Ç–æ–∫–µ–Ω (–∏–ª–∏ HF_TOKEN env var)')
    hf_download_ticks_parser.add_argument('--local-dir', type=str, default='workspace/raw_data/ticks', help='–õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è')
    
    # Hugging Face: Upload training data
    hf_upload_training_parser = subparsers.add_parser('hf-upload-training', help='–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ Hugging Face')
    hf_upload_training_parser.add_argument('--repo-id', type=str, required=True, help='ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (username/dataset-name)')
    hf_upload_training_parser.add_argument('--token', type=str, default=None, help='Hugging Face —Ç–æ–∫–µ–Ω (–∏–ª–∏ HF_TOKEN env var)')
    hf_upload_training_parser.add_argument('--include-scalers', action='store_true', default=True, help='–í–∫–ª—é—á–∏—Ç—å scalers')
    hf_upload_training_parser.add_argument('--no-scalers', action='store_false', dest='include_scalers', help='–ù–µ –≤–∫–ª—é—á–∞—Ç—å scalers')
    hf_upload_training_parser.add_argument('--include-cache', action='store_true', help='–í–∫–ª—é—á–∏—Ç—å –∫—ç—à–∏')
    hf_upload_training_parser.add_argument('--commit-message', type=str, default='Upload training data', help='–°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞')
    
    # Hugging Face: Download training data
    hf_download_training_parser = subparsers.add_parser('hf-download-training', help='–°–∫–∞—á–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å Hugging Face')
    hf_download_training_parser.add_argument('--repo-id', type=str, required=True, help='ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (username/dataset-name)')
    hf_download_training_parser.add_argument('--token', type=str, default=None, help='Hugging Face —Ç–æ–∫–µ–Ω (–∏–ª–∏ HF_TOKEN env var)')
    hf_download_training_parser.add_argument('--local-dir', type=str, default='workspace', help='–õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è')
    
    # Hugging Face: Upload feature analysis
    hf_upload_features_parser = subparsers.add_parser('hf-upload-features', help='–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π –Ω–∞ Hugging Face')
    hf_upload_features_parser.add_argument('--repo-id', type=str, required=True, help='ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (username/dataset-name)')
    hf_upload_features_parser.add_argument('--token', type=str, default=None, help='Hugging Face —Ç–æ–∫–µ–Ω (–∏–ª–∏ HF_TOKEN env var)')
    hf_upload_features_parser.add_argument('--analysis-dir', type=str, default='workspace/analysis-of-features', help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞')
    hf_upload_features_parser.add_argument('--commit-message', type=str, default='Upload feature analysis results', help='–°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞')
    
    # Hugging Face: Download feature analysis
    hf_download_features_parser = subparsers.add_parser('hf-download-features', help='–°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π —Å Hugging Face')
    hf_download_features_parser.add_argument('--repo-id', type=str, required=True, help='ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (username/dataset-name)')
    hf_download_features_parser.add_argument('--token', type=str, default=None, help='Hugging Face —Ç–æ–∫–µ–Ω (–∏–ª–∏ HF_TOKEN env var)')
    hf_download_features_parser.add_argument('--local-dir', type=str, default='workspace/analysis-of-features', help='–õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è')
    
    # Hugging Face: Delete ticks
    hf_delete_ticks_parser = subparsers.add_parser('hf-delete-ticks', help='–£–¥–∞–ª–∏—Ç—å —Ç–∏–∫–∏ –∏–∑ Hugging Face –¥–∞—Ç–∞—Å–µ—Ç–∞')
    hf_delete_ticks_parser.add_argument('--repo-id', type=str, required=True, help='ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (username/dataset-name)')
    hf_delete_ticks_parser.add_argument('--token', type=str, default=None, help='Hugging Face —Ç–æ–∫–µ–Ω (–∏–ª–∏ HF_TOKEN env var)')
    hf_delete_ticks_parser.add_argument('--commit-message', type=str, default='Delete tick data', help='–°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞')
    
    # Hugging Face: Delete training data
    hf_delete_training_parser = subparsers.add_parser('hf-delete-training', help='–£–¥–∞–ª–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏–∑ Hugging Face –¥–∞—Ç–∞—Å–µ—Ç–∞')
    hf_delete_training_parser.add_argument('--repo-id', type=str, required=True, help='ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (username/dataset-name)')
    hf_delete_training_parser.add_argument('--token', type=str, default=None, help='Hugging Face —Ç–æ–∫–µ–Ω (–∏–ª–∏ HF_TOKEN env var)')
    hf_delete_training_parser.add_argument('--include-scalers', action='store_true', default=True, help='–£–¥–∞–ª—è—Ç—å scalers (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –¥–∞)')
    hf_delete_training_parser.add_argument('--no-scalers', action='store_false', dest='include_scalers', help='–ù–µ —É–¥–∞–ª—è—Ç—å scalers')
    hf_delete_training_parser.add_argument('--include-cache', action='store_true', default=True, help='–£–¥–∞–ª—è—Ç—å –∫—ç—à–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –¥–∞)')
    hf_delete_training_parser.add_argument('--no-cache', action='store_false', dest='include_cache', help='–ù–µ —É–¥–∞–ª—è—Ç—å –∫—ç—à–∏')
    hf_delete_training_parser.add_argument('--commit-message', type=str, default='Delete training data', help='–°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞')
    
    # Hugging Face: Delete feature analysis
    hf_delete_features_parser = subparsers.add_parser('hf-delete-features', help='–£–¥–∞–ª–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—á–µ–π –∏–∑ Hugging Face –¥–∞—Ç–∞—Å–µ—Ç–∞')
    hf_delete_features_parser.add_argument('--repo-id', type=str, required=True, help='ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (username/dataset-name)')
    hf_delete_features_parser.add_argument('--token', type=str, default=None, help='Hugging Face —Ç–æ–∫–µ–Ω (–∏–ª–∏ HF_TOKEN env var)')
    hf_delete_features_parser.add_argument('--commit-message', type=str, default='Delete feature analysis results', help='–°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞')
    
    # Hugging Face: Delete all data
    hf_delete_all_parser = subparsers.add_parser('hf-delete-all', help='–£–¥–∞–ª–∏—Ç—å –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ Hugging Face –¥–∞—Ç–∞—Å–µ—Ç–∞ (–æ—á–∏—Å—Ç–∏—Ç—å –¥–ª—è –Ω–æ–≤—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫)')
    hf_delete_all_parser.add_argument('--repo-id', type=str, required=True, help='ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (username/dataset-name)')
    hf_delete_all_parser.add_argument('--token', type=str, default=None, help='Hugging Face —Ç–æ–∫–µ–Ω (–∏–ª–∏ HF_TOKEN env var)')
    hf_delete_all_parser.add_argument('--commit-message', type=str, default='Delete all dataset data', help='–°–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    if args.command == 'upload-training':
        uploader = PaperspaceUploader(host=args.host, path=args.path, user=args.user)
        archive_name = f'training_data_{datetime.now().strftime("%Y%m%d_%H%M%S")}.tar.gz'
        if uploader.create_paperspace_training_archive(archive_name,
                                           include_ticks=args.include_ticks,
                                           include_cache=args.include_cache,
                                           ask_ticks=not args.no_ask_ticks):
            uploader.upload_paperspace_training_data(archive_name, method=args.method)
    
    elif args.command == 'create-training-archive':
        uploader = PaperspaceUploader()
        uploader.create_paperspace_training_archive(
            output_file=args.output,
            include_ticks=args.include_ticks,
            include_cache=args.include_cache,
            ask_ticks=not args.no_ask_ticks
        )
    
    elif args.command == 'download-results':
        downloader = PaperspaceDownloader(host=args.host, user=args.user)
        downloader.download_paperspace_results(args.path, local_path=args.local_path, method=args.method)
    
    elif args.command == 'create-results-archive':
        downloader = PaperspaceDownloader()
        downloader.create_paperspace_results_archive(args.output)
    
    elif args.command == 'list-remote-files':
        downloader = PaperspaceDownloader(host=args.host, path=args.path, user=args.user)
        downloader.list_paperspace_files()
    
    elif args.command == 'hf-upload-ticks':
        if not HF_AVAILABLE:
            print("‚ùå huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
            return
        uploader = HuggingFaceUploader(repo_id=args.repo_id, token=args.token)
        uploader.upload_hf_ticks(ticks_dir=args.ticks_dir, commit_message=args.commit_message)
    
    elif args.command == 'hf-download-ticks':
        if not HF_AVAILABLE:
            print("‚ùå huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
            return
        downloader = HuggingFaceDownloader(repo_id=args.repo_id, token=args.token)
        downloader.download_hf_ticks(local_dir=args.local_dir)
    
    elif args.command == 'hf-upload-training':
        if not HF_AVAILABLE:
            print("‚ùå huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
            return
        uploader = HuggingFaceUploader(repo_id=args.repo_id, token=args.token)
        uploader.upload_hf_training_data(
            include_scalers=args.include_scalers,
            include_cache=args.include_cache,
            commit_message=args.commit_message
        )
    
    elif args.command == 'hf-download-training':
        if not HF_AVAILABLE:
            print("‚ùå huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
            return
        downloader = HuggingFaceDownloader(repo_id=args.repo_id, token=args.token)
        downloader.download_hf_training_data(local_dir=args.local_dir)
    
    elif args.command == 'hf-upload-features':
        if not HF_AVAILABLE:
            print("‚ùå huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
            return
        uploader = HuggingFaceUploader(repo_id=args.repo_id, token=args.token)
        uploader.upload_hf_feature_analysis(
            analysis_dir=args.analysis_dir,
            commit_message=args.commit_message
        )
    
    elif args.command == 'hf-download-features':
        if not HF_AVAILABLE:
            print("‚ùå huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
            return
        downloader = HuggingFaceDownloader(repo_id=args.repo_id, token=args.token)
        downloader.download_hf_feature_analysis(local_dir=args.local_dir)
    
    elif args.command == 'hf-delete-ticks':
        if not HF_AVAILABLE:
            print("‚ùå huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
            return
        deleter = HuggingFaceDeleter(repo_id=args.repo_id, token=args.token)
        deleter.delete_hf_ticks(commit_message=args.commit_message)
    
    elif args.command == 'hf-delete-training':
        if not HF_AVAILABLE:
            print("‚ùå huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
            return
        deleter = HuggingFaceDeleter(repo_id=args.repo_id, token=args.token)
        include_scalers = getattr(args, 'include_scalers', True)
        include_cache = getattr(args, 'include_cache', True)
        deleter.delete_hf_training_data(
            include_scalers=include_scalers,
            include_cache=include_cache,
            commit_message=args.commit_message
        )
    
    elif args.command == 'hf-delete-features':
        if not HF_AVAILABLE:
            print("‚ùå huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
            return
        deleter = HuggingFaceDeleter(repo_id=args.repo_id, token=args.token)
        deleter.delete_hf_feature_analysis(commit_message=args.commit_message)
    
    elif args.command == 'hf-delete-all':
        if not HF_AVAILABLE:
            print("‚ùå huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
            return
        deleter = HuggingFaceDeleter(repo_id=args.repo_id, token=args.token)
        deleter.delete_all_data(commit_message=args.commit_message)
    
    print("\n" + "=" * 60)
    print("–ì–æ—Ç–æ–≤–æ!")
    print("=" * 60)


if __name__ == '__main__':
    main()

